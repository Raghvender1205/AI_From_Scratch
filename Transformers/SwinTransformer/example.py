import numpy as np
import torch
import torch.nn as nn
from transformers import BertTokenizer

"""
This is generated by ChatGPT for Input Text usage (NLP)
"""
class WindowAttention(nn.Module):
    def __init__(self, d_model, nhead):
        super(WindowAttention, self).__init__()
        self.window_attn = nn.MultiheadAttention(d_model, nhead)

    def forward(self, q, k, v):
        x, _ = self.window_attn(q, k, v)
        
        return x
    
class SwinBlock(nn.Module):
    def __init__(self, d_model, nhead, n_layers):
        super(SwinBlock, self).__init__()
        self.window_attn = WindowAttention(d_model, d_model)
        self.feed_forward = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, x):
        x = self.window_attn(x)
        x = self.feed_forward(x)
        
        x = self.dropout(x)
        return x


class SwinTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, n_layers):
        super(SwinTransformer, self).__init__()
        self.emb = nn.Embedding(vocab_size, d_model)
        self.win_attn = WindowAttention(d_model, nhead)
        self.swin_block = SwinBlock(d_model, nhead, n_layers)
        self.fc = nn.Linear(d_model, vocab_size)
    
    def forward(self, x):
        x = self.emb(x)
        x = self.win_attn(x)
        x = self.swin_block(x)
        x = self.fc(x)
    
        return x

if __name__ == '__main__':
    # Hyperparameters
    vocab_size = 1000
    d_model = 512
    nhead = 8
    n_layers = 6
    n_epochs = 10

    model = SwinTransformer(vocab_size, d_model, nhead, n_layers)
    criterion = nn.CrossEntropyLoss()
    opt = torch.optim.Adam(model.parameters())

    # Train
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    input_text = ["The cat sat on the mat", "The dog barked at the mailman"]
    input_ids = tokenizer.encode(input_text)
    
    # print(input_ids)
    input_tensor = torch.tensor(input_ids)
    labels = torch.tensor([0, 1])
    
    for i in range(n_epochs):
        output = model(input_tensor)
        loss = criterion(output, labels)
        
        opt.zero_grad()
        loss.backward()
        opt.step()

        print(f"Loss at epoch {i}: {loss:2f}")