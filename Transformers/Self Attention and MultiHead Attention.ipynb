{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention\n",
    "The concept of `Attention` came from the improvement of `RNN` for handling longer sequences or sentences. For example, consider translating a sentence from one language to another. Translating a sentence `word-by-word` does not work effectively\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/sentence.png\"/>\n",
    "\n",
    "To overcome this issue, `attention mechanisms` were introduced to give access to all sequence elements at each time step. The key is to be selective and determine which words are most important in a specific context. \n",
    "\n",
    "In this, we focus on `Scaled-Dot Product Attention Mechanism (Self Attention)` which is a popular and widely used attention mechanism in practice. There are some other attention mechanisms like `FlashAttention`.\n",
    "\n",
    "## Embedding an Input Sentence\n",
    "Configure an input sentence which would be sent through the `self-attention` mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'21': 0, 'Changotra': 1, 'Hi': 2, 'I': 3, 'My': 4, 'Raghvender': 5, 'am': 6, 'and': 7, 'is': 8, 'name': 9, 'old': 10, 'years': 11}\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Hi, My name is Raghvender Changotra and I am 21 years old'\n",
    "\n",
    "dc = {s: i for i, s in enumerate(sorted(sentence.replace(',', '').split()))}\n",
    "print(dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2,  4,  9,  8,  5,  1,  7,  3,  6,  0, 11, 10])\n"
     ]
    }
   ],
   "source": [
    "# Use this dict to assign an integer index to each word\n",
    "import torch\n",
    "\n",
    "sentence_int = torch.tensor([dc[s] for s in sentence.replace(',', '').split()])\n",
    "print(sentence_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use `Embedding Layer` to encode the inputs into a real-vector embedding. We use a 16-dimensional embedding such that each input word is represented by `16-dimensional` vector. As there are 12 words so this will result in `12x16` embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3250e+00,  1.7843e-01, -2.1338e+00,  1.0524e+00, -3.8848e-01,\n",
      "         -9.3435e-01, -4.9914e-01, -1.0867e+00,  8.8054e-01,  1.5542e+00,\n",
      "          6.2662e-01, -1.7549e-01,  9.8284e-02, -9.3507e-02,  2.6621e-01,\n",
      "         -5.8504e-01],\n",
      "        [ 5.1463e-01,  9.9376e-01, -2.5873e-01, -1.0826e+00, -4.4382e-02,\n",
      "          1.6236e+00, -2.3229e+00,  1.0878e+00,  6.7155e-01,  6.9330e-01,\n",
      "         -9.4872e-01, -7.6507e-02, -1.5264e-01,  1.1674e-01,  4.4026e-01,\n",
      "         -1.4465e+00],\n",
      "        [-1.2743e+00,  4.5128e-01, -2.2801e-01,  9.2238e-01,  2.0561e-01,\n",
      "         -4.9696e-01,  5.8206e-01,  2.0532e-01, -3.0177e-01, -6.7030e-01,\n",
      "         -6.1710e-01, -8.3339e-01,  4.8387e-01, -1.3493e-01,  2.1187e-01,\n",
      "         -8.7140e-01],\n",
      "        [-2.5822e-01, -2.0407e+00, -8.0156e-01, -8.1830e-01, -1.1820e+00,\n",
      "         -2.8774e-01, -6.0430e-01,  6.0024e-01, -1.4053e+00, -5.9217e-01,\n",
      "         -2.5479e-01,  1.1517e+00, -1.7858e-02,  4.2640e-01, -7.6574e-01,\n",
      "         -5.4514e-02],\n",
      "        [ 2.5529e-01, -5.4963e-01,  1.0042e+00,  8.2723e-01, -3.9481e-01,\n",
      "          4.8923e-01, -2.1681e-01, -1.7472e+00, -1.6025e+00, -1.0764e+00,\n",
      "          9.0315e-01, -7.2184e-01, -5.9508e-01, -7.1122e-01,  6.2296e-01,\n",
      "         -1.3729e+00],\n",
      "        [-7.7020e-02, -1.0205e+00, -1.6896e-01,  9.1776e-01,  1.5810e+00,\n",
      "          1.3010e+00,  1.2753e+00, -2.0095e-01,  4.9647e-01, -1.5723e+00,\n",
      "          9.6657e-01, -1.1481e+00, -1.1589e+00,  3.2547e-01, -6.3151e-01,\n",
      "         -2.8400e+00],\n",
      "        [-9.4053e-01, -4.6806e-01,  1.0322e+00, -2.8300e-01,  4.9275e-01,\n",
      "         -1.4078e-02, -2.7466e-01, -7.6409e-01,  1.3966e+00, -9.9491e-01,\n",
      "         -1.5822e-03,  1.2471e+00, -7.7105e-02,  1.2774e+00, -1.4596e+00,\n",
      "         -2.1595e+00],\n",
      "        [ 8.7684e-01,  1.6221e+00, -1.4779e+00,  1.1331e+00, -1.2203e+00,\n",
      "          1.3139e+00,  1.0533e+00,  1.3881e-01,  2.2473e+00, -8.0364e-01,\n",
      "         -2.8084e-01,  7.6968e-01, -6.5956e-01, -7.9793e-01,  1.8383e-01,\n",
      "          2.2935e-01],\n",
      "        [-2.2150e+00, -1.3193e+00, -2.0915e+00,  9.6285e-01, -3.1861e-02,\n",
      "         -4.7896e-01,  7.6681e-01,  2.7468e-02,  1.9929e+00,  1.3708e+00,\n",
      "         -5.0087e-01, -2.7928e-01, -2.0628e+00,  6.3745e-03, -9.8955e-01,\n",
      "          7.0161e-01],\n",
      "        [ 3.3737e-01, -1.7778e-01, -3.0353e-01, -5.8801e-01,  3.4861e-01,\n",
      "          6.6034e-01, -2.1964e-01, -3.7917e-01,  7.6711e-01, -1.1925e+00,\n",
      "          6.9835e-01, -1.4097e+00,  1.7938e-01,  1.8951e+00,  4.9545e-01,\n",
      "          2.6920e-01],\n",
      "        [ 1.5382e-01, -4.4516e-01,  5.5035e-01,  6.5788e-02,  6.8050e-01,\n",
      "          1.2064e+00,  1.6250e+00,  3.4595e-01,  1.3425e-01,  7.6623e-01,\n",
      "          2.2760e+00, -1.3255e+00, -8.9702e-01,  1.1318e-01,  8.3647e-01,\n",
      "          2.8520e-02],\n",
      "        [ 6.8508e-01,  2.0024e+00, -5.4688e-01,  1.6014e+00, -2.2577e+00,\n",
      "         -1.8009e+00,  7.0147e-01,  5.7028e-01, -1.1766e+00, -2.0524e+00,\n",
      "          1.1318e-01,  1.4353e+00,  8.8307e-02, -1.2037e+00,  1.0964e+00,\n",
      "          2.4210e+00]])\n",
      "torch.Size([12, 16])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "emb = torch.nn.Embedding(12, 16)\n",
    "emb_sentence = emb(sentence_int).detach()\n",
    "\n",
    "print(emb_sentence)\n",
    "print(emb_sentence.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Weight Matrices\n",
    "`Self Attention` uses three weight matrices, referred to as $U_q$, $U_k$ and $U_v$, which are adjusted as model parameters during training. These matrices are `query`, `key` and `value` components of the input sequence.\n",
    "\n",
    "These are obtained via matrix multiplication b/w `weight` matrices $U$ and `embedded` inputs $x$\n",
    "- Query Sequence $q^{(i)} = U_qX^{(i)}$ for $i$ $\\epsilon [1, T]$\n",
    "- Key Sequence $k^{(i)} = U_kX^{(i)}$ for $i$ $\\epsilon [1, T]$\n",
    "- Value Sequence $v^{(i)} = U_vX^{(i)}$ for $i$ $\\epsilon [1, T]$\n",
    "\n",
    "The index `i` refers to the token index position in the input sequence, which has length $T$.\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/attention-matrices.png\"/>\n",
    "\n",
    "Here, both $q^{(i)}$ and $k^{(i)}$ are vectors of dimension $d_k$. The projection matrices $U_q$ and $U_k$ have a shape of $d_k * d$, while $U_v$ has the shape $d_v * d$ where $d$ represents the size of each word vector $x$. Since we are computing the `dot-product` b/w `query` and `key` vectors, these vectors have to contain the same number of elements ($d_q = d_k$). However, the number of elements in the value vector $v^{(i)}$ which determines the size of `resulting context vector` is arbitrary.\n",
    "\n",
    "So, here we will set $d_q = d_k = 24$ and use $d_v = 28$ when initializing the projection matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123) # randomize\n",
    "d = emb_sentence.shape[1]\n",
    "d_q, d_k, d_v = 24, 24, 28\n",
    "\n",
    "U_q = torch.randn(d_q, d)\n",
    "U_k = torch.randn(d_k, d)\n",
    "U_v = torch.randn(d_v, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Unormalized Attention Weights\n",
    "For example, we want to calculate the `attention` vector for the second input element. So, $x^{(2)}$ would act as the query.\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/query.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([28])\n"
     ]
    }
   ],
   "source": [
    "# Second Input Element as the query\n",
    "x_2 = emb_sentence[1]\n",
    "q_2 = U_q.matmul(x_2)\n",
    "k_2 = U_k.matmul(x_2)\n",
    "v_2 = U_v.matmul(x_2)\n",
    "\n",
    "print(q_2.shape)\n",
    "print(k_2.shape)\n",
    "print(v_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generalize this to compute the remaining `keys` and `value` elements for all the inputs as we would need them when we compute the unnormalized attention weights $\\omega$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys:  torch.Size([12, 24])\n",
      "Values:  torch.Size([12, 28])\n"
     ]
    }
   ],
   "source": [
    "keys = U_k.matmul(emb_sentence.T).T\n",
    "values = U_v.matmul(emb_sentence.T).T\n",
    "\n",
    "print('Keys: ', keys.shape)\n",
    "print('Values: ', values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the `keys` and `values` to proceed with computation of unnormalized attention weights $\\omega$ as illustrated here\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/omega.png\"/>\n",
    "We compute $\\omega_{ij} = q^{(i)^T}k^{(j)}$ as the `dot product` b/w query and key sequences. For example, we can compute the unnormalized attention weights for the query and  $5th$ input element (index pos -> 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-100.8584)\n"
     ]
    }
   ],
   "source": [
    "omega24 = q_2.dot(keys[4])\n",
    "print(omega24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  63.5880,   95.5014,   -3.3820,  -46.4590, -100.8584,  -98.1709,\n",
      "          -7.3639,    9.3997,   33.1963,   83.1533,   -8.4415,   14.9591])\n"
     ]
    }
   ],
   "source": [
    "# Attention values for all input tokens\n",
    "omega_2 = q_2 @ keys.T\n",
    "print(omega_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Attention Scores\n",
    "\n",
    "The subsequent step in self-attention is to `normalize` the unnormalized attention weights $\\omega$, to obtain the normalized attention weights $\\alpha$, by applying `softmax` function. Additionally, $1/\\sqrt(d_k)$ is used to scale $\\omega$ before normalizing it through `softmax` as shown here\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/attention-scores.png\"/>\n",
    "\n",
    "The scaling by $d_k$ ensures that the `Euclidean length` of the weight vectors will be approx. in the same `magnitude`. This helps prevent the attention weights from becoming too `small` or too `large`, which leads to numerical instablity and might effect model's ability to `converge` during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.2773e-04, 9.5604e-01, 1.7553e-11, 3.6925e-16, 4.5812e-22, 8.9696e-22,\n",
      "        6.4866e-12, 4.2865e-10, 1.6435e-07, 4.3631e-02, 4.9547e-12, 1.7207e-09])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Normalized Attention Weights\n",
    "attn_weights_2 = F.softmax(omega_2 / d**0.5, dim=0)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the last step is to compute the `context` vector $z^{(2)}$, which is an attention-weighted version of our original input query $x^{(2)}$, including all other input elements as its context via `attention weights`.\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/context-vector.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28])\n",
      "tensor([ 0.6245,  3.5794, -3.6488, -4.2545,  1.1219,  2.6817, -3.5003, -6.5867,\n",
      "         1.1002, -5.2203,  0.6476, -0.0411,  0.3919, -0.5379, -2.5635,  1.4992,\n",
      "         0.8038,  0.0308, -4.3862,  5.8357,  0.4138, -0.7649, -3.2749,  0.8404,\n",
      "        -6.0157, -6.9075, -2.3196,  0.3939])\n"
     ]
    }
   ],
   "source": [
    "context_2 = attn_weights_2 @ values\n",
    "\n",
    "print(context_2.shape)\n",
    "print(context_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Head Attention\n",
    "\n",
    "In Multi Head Attention, there are several single head in the context of multi head attention which contain three matrices `query`, `key` and `value` which are transformed from the input sequence.\n",
    "\n",
    "This image illustrates a `Single head Attention` \n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/single-head.png\"/>\n",
    "\n",
    "Now, `MultiHead Attention` contains multiple such heads, each consisting of `query`, `key` and `value` matrices like this.\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/multi-head.png\"/>\n",
    "\n",
    "Now, suppose we have `3` attention heads, so we extend the $d' \\times d$ dimensional weight matrices into $3 \\times d' \\times d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 3\n",
    "mh_U_q = torch.randn(n_heads, d_q, d) # Multihead U_q\n",
    "mh_U_k = torch.randn(n_heads, d_k, d) # Multihead U_k\n",
    "mh_U_v = torch.randn(n_heads, d_v, d) # Multihead U_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each query element is now $3 \\times d_q$ dimensional, where $d_q = 24$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 24])\n"
     ]
    }
   ],
   "source": [
    "# Focus on 3rd element (2nd Index Pos)\n",
    "mh_q_2 = mh_U_q @ x_2\n",
    "\n",
    "print(mh_q_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We obtain keys and values using this\n",
    "mh_k_2 = mh_U_k @ x_2 # keys\n",
    "mh_v_2 = mh_U_v @ x_2 # values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These `keys` and `values` elements are specific to the `query` element. But, we would need the `keys` and `values` for other sequence elements in order to compute the `attention` scores for the query. We can do this by expanding the input sequence `embeddings` to size `3` (No. of Heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 12])\n"
     ]
    }
   ],
   "source": [
    "stacked_inp = emb_sentence.T.repeat(3, 1, 1)\n",
    "\n",
    "print(stacked_inp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `torch.bmm()` for batch matrix multiplication which would enable us to compute all `keys` and `values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHead Keys:  torch.Size([3, 24, 12])\n",
      "MultiHead Values:  torch.Size([3, 28, 12])\n"
     ]
    }
   ],
   "source": [
    "mh_keys = torch.bmm(mh_U_k, stacked_inp)\n",
    "mh_values = torch.bmm(mh_U_v, stacked_inp)\n",
    "\n",
    "print('MultiHead Keys: ', mh_keys.shape)\n",
    "print('MultiHead Values: ', mh_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the tensors to represent the `eight` attention heads in their first dimension. The `3rd` and `2nd` dimension refers to the `no. of words` and `embedding size`. We will swap `2nd` and `3rd` dimensions for better interpretation which would make the tensors having same dimensional structure as the original input sequence `emb_sentence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHead Keys:  torch.Size([3, 12, 24])\n",
      "MultiHead Values:  torch.Size([3, 12, 28])\n"
     ]
    }
   ],
   "source": [
    "mh_keys = mh_keys.permute(0, 2, 1)\n",
    "mh_values = mh_values.permute(0, 2, 1)\n",
    "\n",
    "print('MultiHead Keys: ', mh_keys.shape)\n",
    "print('MultiHead Values: ', mh_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we follow the same steps as above to compute the `unnormalized attention weights` $\\omega$ and `normalized attention weights` $\\alpha$ followed by `scaled-softmax` computation to obtain $h \\times d_v$ `dimensional context vector` $z$ for input element $x^{(2)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
