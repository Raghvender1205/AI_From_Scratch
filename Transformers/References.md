## Link to the latest Annotated Transformer Implementation

http://nlp.seas.harvard.edu/2018/04/03/attention.html

## Link to the OpenNMT Github

https://www.github.com/opennmt/opennmt-py


## Self attention
https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a


## BERT

https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03
https://github.com/google-research/bert
https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270


## Swin Transformer
https://github.com/microsoft/Swin-Transformer

## TODO:
Add Many More...!!!
