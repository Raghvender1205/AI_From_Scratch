{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Analogy\n",
    "Consider the scenario of teaching a dog new tricks. The dog doesn't understand our language, so we can't tell him what to do. Instead, we follow a different strategy. We emulate a situation (or a cue), and the dog tries to respond in many different ways. If the dog's response is the desired one, we reward them with snacks. \n",
    "\n",
    "Now guess what, the next time the dog is exposed to the same situation, the dog executes a similar action with even more enthusiasm in expectation of more food. That's like learning \"what to do\" from positive experiences. Similarly, dogs will tend to learn what not to do when face with negative experiences.\n",
    "\n",
    "That's exactly how Reinforcement Learning works in a broader sense:\n",
    "- Dog is an \"agent\" that is exposed to the environment. \n",
    "- The situations they encounter is `state`. \n",
    "- Our agents react by performing an `action` to transition from on `state` to another.\n",
    "- After the transition, they may recieve a `reward` or `penalty` in return.\n",
    "- The `Policy` is the strategy of choosing an `action` given a `state` in expectation of better outcomes.\n",
    "\n",
    "There are some important things to note:\n",
    "1. <b>Being Greedy does'nt always work</b>:\n",
    "\n",
    "    There are things that are easy to do for instant gratification, and there's things that provide long term rewards  The goal is to not be greedy by looking for the quick immediate rewards, but instead to optimize for maximum rewards over the whole training.\n",
    "    \n",
    "2. <b>Sequence matters in Reinforcement Learning</b>\n",
    "The `reward` agent does not just depend on the current `state`, but the entire history of states.\n",
    "\n",
    "## The Reinforcement Learning Process\n",
    "<img src=\"https://www.learndatasci.com/documents/14/Reinforcement-Learning-Animation.gif\"/>\n",
    "\n",
    "If we break down Reinforcement Learning into steps then:\n",
    "- Observation of the environment\n",
    "- Deciding how to act using some strategy\n",
    "- Acting accordingly\n",
    "- Receiving a reward or penalty\n",
    "- Learning from the experiences and refining our strategy\n",
    "- Iterate until an optimal strategy is found\n",
    "\n",
    "## Example Design: Self Driving Cab\n",
    "In this, we design a self driving cab. The major goal is to demonstrate how to use Reinforment Learning to develop an efficient approach to the problem.\n",
    "\n",
    "The SelfDriving Cab's job is to pick the passenger at one location and drop them off in another. Some few things to take care of\n",
    "- Drop off the passenger to the right location\n",
    "- Save passenger's time by taking minimum time possible to drop off.\n",
    "- Take care of Passenger's safety and traffic rules\n",
    "\n",
    "### 1. Rewards\n",
    "Since the agent (the imaginary driver) is reward-motivated and is going to learn how to control the cab by trial experiences in the environment, we need to decide the `rewards` and/or `penalties` and their magnitude accordingly. Here a few points to consider:\n",
    "\n",
    "- The agent should receive a high positive reward for a successful dropoff because this behavior is highly desired\n",
    "- The agent should be penalized if it tries to drop off a passenger in wrong locations\n",
    "- The agent should get a slight negative reward for not making it to the destination after every time-step. \"Slight\" negative because we would prefer our agent to reach late instead of making wrong moves trying to reach to the destination as fast as possible\n",
    "\n",
    "### 2. State Space\n",
    "In Reinforcement Learning, the `agent` encounters a state, and then takes `action` according to the state it's in.\n",
    "\n",
    "The `State Space` is the set of all possible situations our taxi could inhabit. The state should contain useful information the agent needs to make the right action.\n",
    "\n",
    "Let's say we have a training area for our Smartcab where we are teaching it to transport people in a parking lot to four different locations (R, G, Y, B):\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/lds-media/images/Reinforcement_Learning_Taxi_Env.width-1200.png\"/>\n",
    "\n",
    "Let's assume Smartcab is the only vehicle in this parking lot. We can break up the parking lot into a 5x5 grid, which gives us 25 possible taxi locations. These 25 locations are one part of our state space. Notice the current location state of our taxi is coordinate (3, 1).\n",
    "\n",
    "You'll also notice there are four (4) locations that we can pick up and drop off a passenger: R, G, Y, B or `[(0,0), (0,4), (4,0), (4,3)]` in (row, col) coordinates. Our illustrated passenger is in location `Y` and they wish to go to location `R`.\n",
    "\n",
    "When we also account for one (1) additional passenger state of being inside the taxi, we can take all combinations of passenger locations and destination locations to come to a total number of states for our taxi environment; there's four (4) destinations and five (4 + 1) passenger locations.\n",
    "\n",
    "### 3. Action Space\n",
    "The `agent` encounters one of the 500 `states` and it takes an `action`. The action in our case can be to move in a direction or decide to pickup/dropoff a passenger.\n",
    "\n",
    "In other words, we have six possible actions:\n",
    "- south\n",
    "- north\n",
    "- east\n",
    "- west\n",
    "- pickup\n",
    "- dropoff\n",
    "\n",
    "This is the action space: the set of all the actions that our agent can take in a given state.\n",
    "\n",
    "You'll notice in the illustration above, that the taxi cannot perform certain actions in certain states due to `walls`. In environment's code, we will simply provide a `-1 penalty` for every wall hit and the taxi won't move anywhere. This will just rack up penalties causing the taxi to consider going around the wall.\n",
    "\n",
    "## Implementation\n",
    "We will use `OpenAI Gym` Library for this. Firstly, Install `gym` by using the command\n",
    "```bash\n",
    "pip install gym['atari']\n",
    "```\n",
    "\n",
    "There are some requirements like `cmake` and `scipy` to be installed with it. Once installed, load `Taxi-v2` Game environment and render it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | :\u001b[43m \u001b[0m:G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Load Taxi Environment\n",
    "env = gym.make('Taxi-v3').env # Taxi-v2 -> Not Found\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "Action Space: Discrete(6)\n",
      "State Space: Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "# Reset the Environment to Random State, and print some information\n",
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "print('Action Space: {}'.format(env.action_space))\n",
    "print('State Space: {}'.format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in the environment we have `Action Space` of size 6 and a `State Space` of size 500. The 6 Action States are:\n",
    "\n",
    "* 0 = south\n",
    "* 1 = north\n",
    "* 2 = east\n",
    "* 3 = west\n",
    "* 4 = pickup\n",
    "* 5 = dropoff\n",
    "\n",
    "Recall that the 500 states correspond to a encoding of the taxi's location, the passenger's location, and the destination location.\n",
    "\n",
    "Reinforcement Learning will learn a mapping of `states` to the optimal `action` to perform in that state by exploration, i.e. the agent explores the environment and takes actions based off rewards defined in the environment.\n",
    "\n",
    "The optimal action for each state is the action that has the `highest cumulative long-term reward`.\n",
    "\n",
    "Now, We can encode its `state` and give it to the `env` to render. Recall that we have the taxi at `row 3`, `column 1`, our passenger is at location `2`, and our destination is location `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:  328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi col, passenger index, destination index)\n",
    "print('State: ', state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using our illustration's coordinates to generate a number corresponding to a state between 0 and 499, which turns out to be `328` for our illustration's state.\n",
    "\n",
    "## The Reward Table\n",
    "When the Taxi environment is created, there is an initial Reward table that's also created, called `P`. We can think of it like a matrix that has the number of states as rows and number of actions as columns, i.e. a   `states x actions` matrix.\n",
    "\n",
    "Since every state is in this matrix, we can see the default reward values assigned to our illustration's state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reward Table\n",
    "env.P[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dict has the structure `{action: [(probability, nextstate, reward, done)]}`\n",
    "\n",
    "Some key points:\n",
    "* The 0-5 corresponds to the actions (south, north, east, west, pickup, dropoff) the taxi can perform at our current state in the illustration.\n",
    "* In this env, `probability` is always 1.0.\n",
    "* The `nextstate` is the state we would be in if we take the action at this index of the dict\n",
    "* All the movement actions have a `-1` reward and the pickup/dropoff actions have `-10` reward in this particular state. If we are in a state where the taxi has a passenger and is on top of the right destination, we would see a reward of 20 at the dropoff action.\n",
    "* `done` is used to tell us when we have successfully dropped off a passenger in the right location. Each successfull dropoff is the end of an `episode`\n",
    "\n",
    "Note that if our agent chose to explore action two `(2)` in this `state` it would be going East into a wall. The source code has made it impossible to actually move the taxi across a wall, so if the taxi chooses that action, it will just keep accruing `-1` penalties, which affects the `long-term reward`.\n",
    "\n",
    "## Solving the Problem without Reinforcement Learning\n",
    "For comparison, let's use `BruteForce` to solve the Problem. Since we have our `P` table for default rewards in each `state`, we can try to have our taxi navigate just using that.\n",
    "\n",
    "We'll create an infinite loop which runs until one passenger reaches one `destination` (one episode), or in other words, when the received reward is `20`. The `env.action_space.sample()` method automatically selects one random action from set of all possible actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 999\n",
      "Penalties incurred: 337\n"
     ]
    }
   ],
   "source": [
    "env.s = 328 # Set Env to Illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = []\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for Animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "    })\n",
    "    epochs += 1\n",
    "\n",
    "print(f'Timesteps taken: {epochs}')\n",
    "print(f'Penalties incurred: {penalties}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'getvalue'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3d9ba47e745d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mprint_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-3d9ba47e745d>\u001b[0m in \u001b[0;36mprint_frames\u001b[1;34m(frames)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'frame'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Timestep: {i + 1}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"State: {frame['state']}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'getvalue'"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'].getvalue())\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "\n",
    "\n",
    "print_frames(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02e8b29f27383623620a81b44ff2109e24f067f3b1a958c640937ff95ee3ec72"
  },
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
