{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate some input data (X) consisting of 30 sequences of 8 binary numbers, following a uniform distribution, where the probability of generating a “0” is the same as that of generating a “1”. Make the output (y) for each sequence be the sum of its elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "I  = 30 # Number of training examples\n",
    "T = 8 # Length of the sequence \n",
    "n, p = 1, .5\n",
    "X = np.random.binomial(n, p, (I, T))\n",
    "Y = []\n",
    "for x in X:\n",
    "    Y = np.append(Y, np.sum(x))\n",
    "Y = Y.reshape(I, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_x_bak = 0.0001\n",
    "alpha_f_bak = 0.0001\n",
    "epsilon = 1400\n",
    "\n",
    "delta_init = 0.0003\n",
    "eta_n = 0.5\n",
    "eta_p = 1.2\n",
    "\n",
    "alpha_x_cli = 0.001\n",
    "alpha_f_cli = 0.001\n",
    "eta = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement a Sequential adder using the Elman Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, Vx, Vf):\n",
    "    '''\n",
    "    Function for the forward propogation phase of our RNN returns the \n",
    "    final results after the whole sequence has been treated\n",
    "    '''\n",
    "    Ft = np.zeros(X.shape[0]) # current output we saw\n",
    "    F = np.zeros(X.shape) # to store intermediate values of Ft\n",
    "    for t in range(X.shape[1]):\n",
    "        F[:, t] = Ft\n",
    "        Ft = Vf * Ft + Vx * X[:, t]\n",
    "    return Ft.reshape(X.shape[0], 1), F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BackPropogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sse(Y, predictions):\n",
    "    return 0.5 * np.sum(np.square(predictions - Y))\n",
    "\n",
    "def compute_dE(X, F, Y, predictions, Vx, Vf):\n",
    "    dEVx = 0\n",
    "    dEVf = 0\n",
    "    T = X.shape[1]\n",
    "    for t in range(T):\n",
    "        sx = 0\n",
    "        sf = 0\n",
    "        for i, x in enumerate(X):\n",
    "            sx += (predictions[i] - Y[i]) * X[t]\n",
    "            sf += (predictions[i] - Y[i]) * F[i][t]\n",
    "        c = (Vf ** (T-t+1))\n",
    "        dEVx += sx * c\n",
    "        dEVf += sf * c\n",
    "    return dEVx, dEVf\n",
    "\n",
    "\n",
    "def backward(X, F, Y, predictions, Vx, Vf, alphax, alphaf):\n",
    "    '''\n",
    "    Vanilla Backpropogation Function, it is called at each batch or Forward pass \n",
    "    it updates the values of the parameters and returns Vf and Vx\n",
    "    '''\n",
    "    dEVx, dEVf = compute_dE(X, F, Y, predictions, Vx, Vf)\n",
    "    Vx -= (alphax * dEVx)\n",
    "    Vf -= (alphaf * dEVf)\n",
    "    print(\"dEVx :\",dEVx,\"dEVf :\",dEVf)\n",
    "    print(\"Vx :\",Vx,\"Vf :\",Vf)\n",
    "    return Vx, Vf, dEVx, dEVf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Function for the BackPropogation Algorithm\n",
    "# Outputs the optimal weights and the errors during training \n",
    "\n",
    "def train_backward(X, Y, alphax, alphaf, epsilon, Vx, Vf):\n",
    "    SSE_arr = []    #Array to store all SSE over time\n",
    "    Vx_evo = [] #Array to store all Vx over time\n",
    "    Vf_evo = [] #Array to store all Vf over time\n",
    "    dEVx_evo = [] #Array to store all dEVx over time\n",
    "    dEVf_evo = [] #Array to store all dEVf over time\n",
    "    \n",
    "    diff_error = epsilon+1\n",
    "    prev_diff_error = diff_error\n",
    "    prev_error = 0\n",
    "    \n",
    "    n_iter = 0\n",
    "    stop_loop = 0\n",
    "    for p in range(400):\n",
    "        n_iter += 1\n",
    "        predictions, F = forward(X, Vx, Vf)\n",
    "        cur_error = SSE(Y, predictions)\n",
    "        SSE_arr = np.append(SSE, cur_error)\n",
    "        \n",
    "        Vf, Vx, dEVx, dEVf = backward(X, F, Y, predictions, Vx, Vf, alphax, alphaf)\n",
    "        Vx_evo = np.append(Vx_evo,Vx)\n",
    "        Vf_evo = np.append(Vf_evo,Vf)\n",
    "        dEVx_evo = np.append(dEVx_evo,dEVx)\n",
    "        dEVf_evo = np.append(dEVf_evo,dEVf)\n",
    "        \n",
    "        prev_diff_error = diff_error\n",
    "        if (n_iter > 1):\n",
    "            diff_error = abs(cur_error - prev_error)\n",
    "        prev_error = cur_error\n",
    "        print(\"Training #\",n_iter,\" Diff SSE: \",diff_error, \" SSE : \",cur_error)\n",
    "        print(\"dEVx :\",dEVx,\"dEVf :\",dEVf)\n",
    "        print(\"Vx :\",Vx,\"Vf :\",Vf)\n",
    "        print(\"__________________________________________________________________\")\n",
    "         #if the current and previous error difference is lesser than espilon, increment the stopping variable\n",
    "        if(cur_error < epsilon):\n",
    "            stop_loop+=1\n",
    "        else :\n",
    "            stop_loop = 0\n",
    "        \n",
    "    print(\"Finished after {} iterations, finals weights Vx {} Vf {}\".format(n_iter,Vx,Vf))\n",
    "    return Vx_evo,Vf_evo,dEVx_evo,dEVf_evo,SSE_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resilient Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a resilient backpropogation model\n",
    "# Return: Outputs the optimal weights and the errors during training \n",
    "def train_resilient(X, y, delta_init, eta_p, eta_n, epsilon, Vx, Vf):\n",
    "    SSE = []    #Array to store all SSE over time\n",
    "    Vx_evo = [] #Array to store all Vx over time\n",
    "    Vf_evo = [] #Array to store all Vf over time\n",
    "    dEVx_evo = [] #Array to store all dEVx over time\n",
    "    dEVf_evo = [] #Array to store all dEVf over time\n",
    "    \n",
    "    delta_f = delta_init\n",
    "    delta_x = delta_init\n",
    "    diff_error = 0.0\n",
    "    prev_diff_error = diff_error\n",
    "    prev_error = 0\n",
    "    cur_error = 0\n",
    "    dEVx,dEVf = 0,0\n",
    "    \n",
    "    stop_loop=0\n",
    "    n_iter=0\n",
    "    \n",
    "    for p in range(400):\n",
    "        # Loop while we don't have three consecutive error differences lesser than epsilon, meaning we converged \n",
    "        \n",
    "        n_iter += 1\n",
    "        predictions, F = forward(X, Vx, Vf)\n",
    "        \n",
    "        # Save previous derivative for sign comparison\n",
    "        prev_dEVx = dEVx\n",
    "        prev_dEVf=dEVf\n",
    "        \n",
    "        dEVx,dEVf = compute_dE(X,F,Y,predictions,Vx,Vf)\n",
    "        \n",
    "        if np.sign()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
