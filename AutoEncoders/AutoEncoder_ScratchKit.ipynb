{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import progressbar\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "from scratchkit.dl.optim import Adam\n",
    "from scratchkit.dl.losses import CrossEntropy, SquareLoss\n",
    "from scratchkit.dl.layers import Dense, Dropout, Flatten, Activation, Reshape, BatchNormalization\n",
    "from scratchkit.dl import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder():\n",
    "    \"\"\"\n",
    "    An AutoEncoder with deep-connected neural nets\n",
    "    \n",
    "    Training data: MNIST\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.img_dim = self.img_rows * self.img_cols\n",
    "        self.latent_dim = 128 # The dimension of the data embedding\n",
    "\n",
    "        optimizer = Adam(learning_rate=0.0002, b1=0.5)\n",
    "        loss_function = SquareLoss\n",
    "\n",
    "        self.encoder = self.build_encoder(optimizer, loss_function)\n",
    "        self.decoder = self.build_decoder(optimizer, loss_function)\n",
    "\n",
    "        self.autoencoder = NeuralNetwork(optimizer=optimizer, loss=loss_function)\n",
    "        self.autoencoder.layers.extend(self.encoder.layers)\n",
    "        self.autoencoder.layers.extend(self.decoder.layers)\n",
    "\n",
    "        print ()\n",
    "        self.autoencoder.summary(name=\"Variational Autoencoder\")\n",
    "\n",
    "    def build_encoder(self, optimizer, loss_function):\n",
    "\n",
    "        encoder = NeuralNetwork(optimizer=optimizer, loss=loss_function)\n",
    "        encoder.add(Dense(512, input_shape=(self.img_dim,)))\n",
    "        encoder.add(Activation('leaky_relu'))\n",
    "        encoder.add(BatchNormalization(momentum=0.8))\n",
    "        encoder.add(Dense(256))\n",
    "        encoder.add(Activation('leaky_relu'))\n",
    "        encoder.add(BatchNormalization(momentum=0.8))\n",
    "        encoder.add(Dense(self.latent_dim))\n",
    "\n",
    "        return encoder\n",
    "\n",
    "    def build_decoder(self, optimizer, loss_function):\n",
    "\n",
    "        decoder = NeuralNetwork(optimizer=optimizer, loss=loss_function)\n",
    "        decoder.add(Dense(256, input_shape=(self.latent_dim,)))\n",
    "        decoder.add(Activation('leaky_relu'))\n",
    "        decoder.add(BatchNormalization(momentum=0.8))\n",
    "        decoder.add(Dense(512))\n",
    "        decoder.add(Activation('leaky_relu'))\n",
    "        decoder.add(BatchNormalization(momentum=0.8))\n",
    "        decoder.add(Dense(self.img_dim))\n",
    "        decoder.add(Activation('tanh'))\n",
    "\n",
    "        return decoder\n",
    "\n",
    "    def train(self, n_epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        mnist = fetch_openml('mnist_784')\n",
    "\n",
    "        X = mnist.data\n",
    "        y = mnist.target\n",
    "\n",
    "        # Rescale [-1, 1]\n",
    "        X = (X.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X.shape[0], batch_size)\n",
    "            imgs = X[idx]\n",
    "\n",
    "            # Train the Autoencoder\n",
    "            loss, _ = self.autoencoder.train_on_batch(imgs, imgs)\n",
    "\n",
    "            # Display the progress\n",
    "            print (\"%d [D loss: %f]\" % (epoch, loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch, X)\n",
    "\n",
    "    def save_imgs(self, epoch, X):\n",
    "        r, c = 5, 5 # Grid size\n",
    "        # Select a random half batch of images\n",
    "        idx = np.random.randint(0, X.shape[0], r*c)\n",
    "        imgs = X[idx]\n",
    "        # Generate images and reshape to image shape\n",
    "        gen_imgs = self.autoencoder.predict(imgs).reshape((-1, self.img_rows, self.img_cols))\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        plt.suptitle(\"Autoencoder\")\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt,:,:], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"ae_%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+-------------------------+\n",
      "| Variational Autoencoder |\n",
      "+-------------------------+\n",
      "Input Shape: (784,)\n",
      "+------------------------+------------+--------------+\n",
      "| Layer Type             | Parameters | Output Shape |\n",
      "+------------------------+------------+--------------+\n",
      "| Dense                  | 401920     | (512,)       |\n",
      "| Activation (LeakyReLU) | 0          | (512,)       |\n",
      "| BatchNormalization     | 1024       | (512,)       |\n",
      "| Dense                  | 131328     | (256,)       |\n",
      "| Activation (LeakyReLU) | 0          | (256,)       |\n",
      "| BatchNormalization     | 512        | (256,)       |\n",
      "| Dense                  | 32896      | (128,)       |\n",
      "| Dense                  | 33024      | (256,)       |\n",
      "| Activation (LeakyReLU) | 0          | (256,)       |\n",
      "| BatchNormalization     | 512        | (256,)       |\n",
      "| Dense                  | 131584     | (512,)       |\n",
      "| Activation (LeakyReLU) | 0          | (512,)       |\n",
      "| BatchNormalization     | 1024       | (512,)       |\n",
      "| Dense                  | 402192     | (784,)       |\n",
      "| Activation (Tanh)      | 0          | (784,)       |\n",
      "+------------------------+------------+--------------+\n",
      "Total Parameters: 1136016\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: N/A% [-                                              ] ETA:  --:--:--\n",
      "Training: N/A% [-                                              ] ETA:  --:--:--\n",
      "Training: N/A% [-                                              ] ETA:  --:--:--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.560906]\n",
      "1 [D loss: 0.551484]\n",
      "2 [D loss: 0.545883]\n",
      "3 [D loss: 0.535760]\n",
      "4 [D loss: 0.527998]\n",
      "5 [D loss: 0.520596]\n",
      "6 [D loss: 0.516944]\n",
      "7 [D loss: 0.509262]\n",
      "8 [D loss: 0.500547]\n",
      "9 [D loss: 0.494367]\n",
      "10 [D loss: 0.493100]\n",
      "11 [D loss: 0.491192]\n",
      "12 [D loss: 0.481135]\n",
      "13 [D loss: 0.477259]\n",
      "14 [D loss: 0.481425]\n",
      "15 [D loss: 0.476635]\n",
      "16 [D loss: 0.470372]\n",
      "17 [D loss: 0.470374]\n",
      "18 [D loss: 0.465281]\n",
      "19 [D loss: 0.467958]\n",
      "20 [D loss: 0.459608]\n",
      "21 [D loss: 0.459647]\n",
      "22 [D loss: 0.453013]\n",
      "23 [D loss: 0.449364]\n",
      "24 [D loss: 0.446528]\n",
      "25 [D loss: 0.452391]\n",
      "26 [D loss: 0.450766]\n",
      "27 [D loss: 0.445012]\n",
      "28 [D loss: 0.445116]\n",
      "29 [D loss: 0.439396]\n",
      "30 [D loss: 0.440218]\n",
      "31 [D loss: 0.440462]\n",
      "32 [D loss: 0.439656]\n",
      "33 [D loss: 0.437230]\n",
      "34 [D loss: 0.440809]\n",
      "35 [D loss: 0.438820]\n",
      "36 [D loss: 0.434369]\n",
      "37 [D loss: 0.430145]\n",
      "38 [D loss: 0.440041]\n",
      "39 [D loss: 0.437350]\n",
      "40 [D loss: 0.426140]\n",
      "41 [D loss: 0.425560]\n",
      "42 [D loss: 0.427967]\n",
      "43 [D loss: 0.430840]\n",
      "44 [D loss: 0.422797]\n",
      "45 [D loss: 0.427004]\n",
      "46 [D loss: 0.435994]\n",
      "47 [D loss: 0.419949]\n",
      "48 [D loss: 0.424919]\n",
      "49 [D loss: 0.424009]\n",
      "50 [D loss: 0.421026]\n",
      "51 [D loss: 0.420523]\n",
      "52 [D loss: 0.413168]\n",
      "53 [D loss: 0.417739]\n",
      "54 [D loss: 0.410254]\n",
      "55 [D loss: 0.420831]\n",
      "56 [D loss: 0.417002]\n",
      "57 [D loss: 0.411903]\n",
      "58 [D loss: 0.409100]\n",
      "59 [D loss: 0.419817]\n",
      "60 [D loss: 0.423238]\n",
      "61 [D loss: 0.415434]\n",
      "62 [D loss: 0.409733]\n",
      "63 [D loss: 0.404106]\n",
      "64 [D loss: 0.412680]\n",
      "65 [D loss: 0.412480]\n",
      "66 [D loss: 0.409015]\n",
      "67 [D loss: 0.411331]\n",
      "68 [D loss: 0.412114]\n",
      "69 [D loss: 0.407275]\n",
      "70 [D loss: 0.403957]\n",
      "71 [D loss: 0.406023]\n",
      "72 [D loss: 0.406976]\n",
      "73 [D loss: 0.410137]\n",
      "74 [D loss: 0.398612]\n",
      "75 [D loss: 0.417595]\n",
      "76 [D loss: 0.401428]\n",
      "77 [D loss: 0.412304]\n",
      "78 [D loss: 0.398270]\n",
      "79 [D loss: 0.398568]\n",
      "80 [D loss: 0.402114]\n",
      "81 [D loss: 0.408112]\n",
      "82 [D loss: 0.391864]\n",
      "83 [D loss: 0.402413]\n",
      "84 [D loss: 0.399357]\n",
      "85 [D loss: 0.403850]\n",
      "86 [D loss: 0.397723]\n",
      "87 [D loss: 0.397054]\n",
      "88 [D loss: 0.395990]\n",
      "89 [D loss: 0.397890]\n",
      "90 [D loss: 0.397627]\n",
      "91 [D loss: 0.396218]\n",
      "92 [D loss: 0.398889]\n",
      "93 [D loss: 0.393471]\n",
      "94 [D loss: 0.390392]\n",
      "95 [D loss: 0.404811]\n",
      "96 [D loss: 0.391387]\n",
      "97 [D loss: 0.391334]\n",
      "98 [D loss: 0.398641]\n",
      "99 [D loss: 0.386054]\n",
      "100 [D loss: 0.401166]\n",
      "101 [D loss: 0.391265]\n",
      "102 [D loss: 0.387794]\n",
      "103 [D loss: 0.390138]\n",
      "104 [D loss: 0.387319]\n",
      "105 [D loss: 0.386512]\n",
      "106 [D loss: 0.391340]\n",
      "107 [D loss: 0.392156]\n",
      "108 [D loss: 0.390362]\n",
      "109 [D loss: 0.398844]\n",
      "110 [D loss: 0.386720]\n",
      "111 [D loss: 0.386613]\n",
      "112 [D loss: 0.382320]\n",
      "113 [D loss: 0.390894]\n",
      "114 [D loss: 0.390716]\n",
      "115 [D loss: 0.392175]\n",
      "116 [D loss: 0.388238]\n",
      "117 [D loss: 0.384190]\n",
      "118 [D loss: 0.383671]\n",
      "119 [D loss: 0.385648]\n",
      "120 [D loss: 0.383987]\n",
      "121 [D loss: 0.386330]\n",
      "122 [D loss: 0.380381]\n",
      "123 [D loss: 0.375979]\n",
      "124 [D loss: 0.381088]\n",
      "125 [D loss: 0.386170]\n",
      "126 [D loss: 0.380567]\n",
      "127 [D loss: 0.389174]\n",
      "128 [D loss: 0.381019]\n",
      "129 [D loss: 0.388646]\n",
      "130 [D loss: 0.382909]\n",
      "131 [D loss: 0.382380]\n",
      "132 [D loss: 0.382208]\n",
      "133 [D loss: 0.385174]\n",
      "134 [D loss: 0.378797]\n",
      "135 [D loss: 0.387941]\n",
      "136 [D loss: 0.383012]\n",
      "137 [D loss: 0.381634]\n",
      "138 [D loss: 0.376031]\n",
      "139 [D loss: 0.383821]\n",
      "140 [D loss: 0.376360]\n",
      "141 [D loss: 0.378156]\n",
      "142 [D loss: 0.367115]\n",
      "143 [D loss: 0.371589]\n",
      "144 [D loss: 0.369562]\n",
      "145 [D loss: 0.374932]\n",
      "146 [D loss: 0.371711]\n",
      "147 [D loss: 0.378021]\n",
      "148 [D loss: 0.373343]\n",
      "149 [D loss: 0.378499]\n",
      "150 [D loss: 0.373189]\n",
      "151 [D loss: 0.375163]\n",
      "152 [D loss: 0.389991]\n",
      "153 [D loss: 0.380710]\n",
      "154 [D loss: 0.383440]\n",
      "155 [D loss: 0.376719]\n",
      "156 [D loss: 0.370206]\n",
      "157 [D loss: 0.378211]\n",
      "158 [D loss: 0.374069]\n",
      "159 [D loss: 0.365660]\n",
      "160 [D loss: 0.367855]\n",
      "161 [D loss: 0.370541]\n",
      "162 [D loss: 0.368989]\n",
      "163 [D loss: 0.365679]\n",
      "164 [D loss: 0.372329]\n",
      "165 [D loss: 0.360783]\n",
      "166 [D loss: 0.373281]\n",
      "167 [D loss: 0.367626]\n",
      "168 [D loss: 0.369602]\n",
      "169 [D loss: 0.400369]\n",
      "170 [D loss: 0.369303]\n",
      "171 [D loss: 0.364244]\n",
      "172 [D loss: 0.372261]\n",
      "173 [D loss: 0.364450]\n",
      "174 [D loss: 0.364554]\n",
      "175 [D loss: 0.362079]\n",
      "176 [D loss: 0.361112]\n",
      "177 [D loss: 0.362473]\n",
      "178 [D loss: 0.365390]\n",
      "179 [D loss: 0.362031]\n",
      "180 [D loss: 0.366699]\n",
      "181 [D loss: 0.364153]\n",
      "182 [D loss: 0.365945]\n",
      "183 [D loss: 0.356708]\n",
      "184 [D loss: 0.375427]\n",
      "185 [D loss: 0.360340]\n",
      "186 [D loss: 0.361624]\n",
      "187 [D loss: 0.373845]\n",
      "188 [D loss: 0.364353]\n",
      "189 [D loss: 0.364740]\n",
      "190 [D loss: 0.359347]\n",
      "191 [D loss: 0.358964]\n",
      "192 [D loss: 0.357571]\n",
      "193 [D loss: 0.369234]\n",
      "194 [D loss: 0.378328]\n",
      "195 [D loss: 0.361425]\n",
      "196 [D loss: 0.357362]\n",
      "197 [D loss: 0.365553]\n",
      "198 [D loss: 0.368043]\n",
      "199 [D loss: 0.368935]\n",
      "200 [D loss: 0.362881]\n",
      "201 [D loss: 0.357248]\n",
      "202 [D loss: 0.356201]\n",
      "203 [D loss: 0.362817]\n",
      "204 [D loss: 0.358506]\n",
      "205 [D loss: 0.354696]\n",
      "206 [D loss: 0.367493]\n",
      "207 [D loss: 0.358369]\n",
      "208 [D loss: 0.363906]\n",
      "209 [D loss: 0.353304]\n",
      "210 [D loss: 0.381959]\n",
      "211 [D loss: 0.362298]\n",
      "212 [D loss: 0.357457]\n",
      "213 [D loss: 0.356147]\n",
      "214 [D loss: 0.358152]\n",
      "215 [D loss: 0.357888]\n",
      "216 [D loss: 0.356752]\n",
      "217 [D loss: 0.358241]\n",
      "218 [D loss: 0.357016]\n",
      "219 [D loss: 0.359045]\n",
      "220 [D loss: 0.367450]\n",
      "221 [D loss: 0.358439]\n",
      "222 [D loss: 0.356361]\n",
      "223 [D loss: 0.364008]\n",
      "224 [D loss: 0.354897]\n",
      "225 [D loss: 0.358179]\n",
      "226 [D loss: 0.349039]\n",
      "227 [D loss: 0.351391]\n",
      "228 [D loss: 0.357948]\n",
      "229 [D loss: 0.353802]\n",
      "230 [D loss: 0.352564]\n",
      "231 [D loss: 0.360838]\n",
      "232 [D loss: 0.356207]\n",
      "233 [D loss: 0.349938]\n",
      "234 [D loss: 0.351217]\n",
      "235 [D loss: 0.349855]\n",
      "236 [D loss: 0.351474]\n",
      "237 [D loss: 0.350026]\n",
      "238 [D loss: 0.347510]\n",
      "239 [D loss: 0.354602]\n",
      "240 [D loss: 0.354690]\n",
      "241 [D loss: 0.348231]\n",
      "242 [D loss: 0.349179]\n",
      "243 [D loss: 0.359610]\n",
      "244 [D loss: 0.355682]\n",
      "245 [D loss: 0.357477]\n",
      "246 [D loss: 0.354071]\n",
      "247 [D loss: 0.350410]\n",
      "248 [D loss: 0.346271]\n",
      "249 [D loss: 0.347291]\n",
      "250 [D loss: 0.347049]\n",
      "251 [D loss: 0.346249]\n",
      "252 [D loss: 0.348600]\n",
      "253 [D loss: 0.351314]\n",
      "254 [D loss: 0.364875]\n",
      "255 [D loss: 0.345542]\n",
      "256 [D loss: 0.363039]\n",
      "257 [D loss: 0.347808]\n",
      "258 [D loss: 0.358433]\n",
      "259 [D loss: 0.352296]\n",
      "260 [D loss: 0.349389]\n",
      "261 [D loss: 0.345123]\n",
      "262 [D loss: 0.356768]\n",
      "263 [D loss: 0.344205]\n",
      "264 [D loss: 0.350882]\n",
      "265 [D loss: 0.342956]\n",
      "266 [D loss: 0.356815]\n",
      "267 [D loss: 0.339534]\n",
      "268 [D loss: 0.338770]\n",
      "269 [D loss: 0.348366]\n",
      "270 [D loss: 0.339770]\n",
      "271 [D loss: 0.349257]\n",
      "272 [D loss: 0.345238]\n",
      "273 [D loss: 0.351471]\n",
      "274 [D loss: 0.344448]\n",
      "275 [D loss: 0.347862]\n",
      "276 [D loss: 0.340825]\n",
      "277 [D loss: 0.340067]\n",
      "278 [D loss: 0.341991]\n",
      "279 [D loss: 0.354174]\n",
      "280 [D loss: 0.348032]\n",
      "281 [D loss: 0.339423]\n",
      "282 [D loss: 0.336358]\n",
      "283 [D loss: 0.338147]\n",
      "284 [D loss: 0.343424]\n",
      "285 [D loss: 0.337588]\n",
      "286 [D loss: 0.349435]\n",
      "287 [D loss: 0.345987]\n",
      "288 [D loss: 0.341485]\n",
      "289 [D loss: 0.337396]\n",
      "290 [D loss: 0.341875]\n",
      "291 [D loss: 0.347282]\n",
      "292 [D loss: 0.344258]\n",
      "293 [D loss: 0.341037]\n",
      "294 [D loss: 0.341057]\n",
      "295 [D loss: 0.341730]\n",
      "296 [D loss: 0.341476]\n",
      "297 [D loss: 0.341241]\n",
      "298 [D loss: 0.355676]\n",
      "299 [D loss: 0.364733]\n",
      "300 [D loss: 0.343683]\n",
      "301 [D loss: 0.334187]\n",
      "302 [D loss: 0.341377]\n",
      "303 [D loss: 0.337858]\n",
      "304 [D loss: 0.337171]\n",
      "305 [D loss: 0.341486]\n",
      "306 [D loss: 0.345326]\n",
      "307 [D loss: 0.331828]\n",
      "308 [D loss: 0.342362]\n",
      "309 [D loss: 0.358345]\n",
      "310 [D loss: 0.343027]\n",
      "311 [D loss: 0.354563]\n",
      "312 [D loss: 0.334980]\n",
      "313 [D loss: 0.333063]\n",
      "314 [D loss: 0.338954]\n",
      "315 [D loss: 0.337803]\n",
      "316 [D loss: 0.333360]\n",
      "317 [D loss: 0.349277]\n",
      "318 [D loss: 0.332409]\n",
      "319 [D loss: 0.334434]\n",
      "320 [D loss: 0.342516]\n",
      "321 [D loss: 0.335052]\n",
      "322 [D loss: 0.337930]\n",
      "323 [D loss: 0.326518]\n",
      "324 [D loss: 0.330129]\n",
      "325 [D loss: 0.343915]\n",
      "326 [D loss: 0.355655]\n",
      "327 [D loss: 0.326492]\n",
      "328 [D loss: 0.333563]\n",
      "329 [D loss: 0.337212]\n",
      "330 [D loss: 0.338855]\n",
      "331 [D loss: 0.335921]\n",
      "332 [D loss: 0.331037]\n",
      "333 [D loss: 0.336551]\n",
      "334 [D loss: 0.333740]\n",
      "335 [D loss: 0.335026]\n",
      "336 [D loss: 0.337629]\n",
      "337 [D loss: 0.332092]\n",
      "338 [D loss: 0.337292]\n",
      "339 [D loss: 0.331042]\n",
      "340 [D loss: 0.328271]\n",
      "341 [D loss: 0.335558]\n",
      "342 [D loss: 0.326471]\n",
      "343 [D loss: 0.328522]\n",
      "344 [D loss: 0.331293]\n",
      "345 [D loss: 0.328154]\n",
      "346 [D loss: 0.325407]\n",
      "347 [D loss: 0.327800]\n",
      "348 [D loss: 0.335770]\n",
      "349 [D loss: 0.342079]\n",
      "350 [D loss: 0.332736]\n",
      "351 [D loss: 0.333710]\n",
      "352 [D loss: 0.335102]\n",
      "353 [D loss: 0.351086]\n",
      "354 [D loss: 0.328555]\n",
      "355 [D loss: 0.328545]\n",
      "356 [D loss: 0.325792]\n",
      "357 [D loss: 0.327251]\n",
      "358 [D loss: 0.361312]\n",
      "359 [D loss: 0.324327]\n",
      "360 [D loss: 0.325521]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361 [D loss: 0.336948]\n",
      "362 [D loss: 0.325840]\n",
      "363 [D loss: 0.344711]\n",
      "364 [D loss: 0.337073]\n",
      "365 [D loss: 0.317330]\n",
      "366 [D loss: 0.331502]\n",
      "367 [D loss: 0.335071]\n",
      "368 [D loss: 0.329961]\n",
      "369 [D loss: 0.329408]\n",
      "370 [D loss: 0.345480]\n",
      "371 [D loss: 0.327159]\n",
      "372 [D loss: 0.325845]\n",
      "373 [D loss: 0.340448]\n",
      "374 [D loss: 0.330513]\n",
      "375 [D loss: 0.325974]\n",
      "376 [D loss: 0.331535]\n",
      "377 [D loss: 0.326588]\n",
      "378 [D loss: 0.337907]\n",
      "379 [D loss: 0.327959]\n",
      "380 [D loss: 0.326471]\n",
      "381 [D loss: 0.325710]\n",
      "382 [D loss: 0.322307]\n",
      "383 [D loss: 0.332611]\n",
      "384 [D loss: 0.317306]\n",
      "385 [D loss: 0.323204]\n",
      "386 [D loss: 0.329101]\n",
      "387 [D loss: 0.319455]\n",
      "388 [D loss: 0.326013]\n",
      "389 [D loss: 0.321019]\n",
      "390 [D loss: 0.322160]\n",
      "391 [D loss: 0.327345]\n",
      "392 [D loss: 0.325745]\n",
      "393 [D loss: 0.334736]\n",
      "394 [D loss: 0.323122]\n",
      "395 [D loss: 0.322082]\n",
      "396 [D loss: 0.317390]\n",
      "397 [D loss: 0.326995]\n",
      "398 [D loss: 0.325722]\n",
      "399 [D loss: 0.325633]\n",
      "400 [D loss: 0.329627]\n",
      "401 [D loss: 0.318654]\n",
      "402 [D loss: 0.323271]\n",
      "403 [D loss: 0.320809]\n",
      "404 [D loss: 0.325581]\n",
      "405 [D loss: 0.321860]\n",
      "406 [D loss: 0.350014]\n",
      "407 [D loss: 0.316685]\n",
      "408 [D loss: 0.382285]\n",
      "409 [D loss: 0.317872]\n",
      "410 [D loss: 0.317544]\n",
      "411 [D loss: 0.318916]\n",
      "412 [D loss: 0.325952]\n",
      "413 [D loss: 0.322788]\n",
      "414 [D loss: 0.313513]\n",
      "415 [D loss: 0.329319]\n",
      "416 [D loss: 0.337161]\n",
      "417 [D loss: 0.322993]\n",
      "418 [D loss: 0.319315]\n",
      "419 [D loss: 0.317422]\n",
      "420 [D loss: 0.314677]\n",
      "421 [D loss: 0.323720]\n",
      "422 [D loss: 0.326526]\n",
      "423 [D loss: 0.313624]\n",
      "424 [D loss: 0.318157]\n",
      "425 [D loss: 0.319665]\n",
      "426 [D loss: 0.310797]\n",
      "427 [D loss: 0.321945]\n",
      "428 [D loss: 0.312153]\n",
      "429 [D loss: 0.312817]\n",
      "430 [D loss: 0.316950]\n",
      "431 [D loss: 0.314655]\n",
      "432 [D loss: 0.321075]\n",
      "433 [D loss: 0.321763]\n",
      "434 [D loss: 0.331631]\n",
      "435 [D loss: 0.315477]\n",
      "436 [D loss: 0.317857]\n",
      "437 [D loss: 0.325214]\n",
      "438 [D loss: 0.321431]\n",
      "439 [D loss: 0.343880]\n",
      "440 [D loss: 0.317735]\n",
      "441 [D loss: 0.320887]\n",
      "442 [D loss: 0.329202]\n",
      "443 [D loss: 0.316631]\n",
      "444 [D loss: 0.321672]\n",
      "445 [D loss: 0.326345]\n",
      "446 [D loss: 0.316155]\n",
      "447 [D loss: 0.317098]\n",
      "448 [D loss: 0.316949]\n",
      "449 [D loss: 0.318102]\n",
      "450 [D loss: 0.318149]\n",
      "451 [D loss: 0.316107]\n",
      "452 [D loss: 0.307090]\n",
      "453 [D loss: 0.307673]\n",
      "454 [D loss: 0.314156]\n",
      "455 [D loss: 0.312627]\n",
      "456 [D loss: 0.308332]\n",
      "457 [D loss: 0.313497]\n",
      "458 [D loss: 0.309448]\n",
      "459 [D loss: 0.315815]\n",
      "460 [D loss: 0.309519]\n",
      "461 [D loss: 0.314420]\n",
      "462 [D loss: 0.303463]\n",
      "463 [D loss: 0.319537]\n",
      "464 [D loss: 0.314113]\n",
      "465 [D loss: 0.311586]\n",
      "466 [D loss: 0.310629]\n",
      "467 [D loss: 0.315626]\n",
      "468 [D loss: 0.317658]\n",
      "469 [D loss: 0.319381]\n",
      "470 [D loss: 0.317001]\n",
      "471 [D loss: 0.310877]\n",
      "472 [D loss: 0.317847]\n",
      "473 [D loss: 0.303679]\n",
      "474 [D loss: 0.309855]\n",
      "475 [D loss: 0.314858]\n",
      "476 [D loss: 0.317619]\n",
      "477 [D loss: 0.310007]\n",
      "478 [D loss: 0.307693]\n",
      "479 [D loss: 0.310023]\n",
      "480 [D loss: 0.309743]\n",
      "481 [D loss: 0.312365]\n",
      "482 [D loss: 0.305030]\n",
      "483 [D loss: 0.313748]\n",
      "484 [D loss: 0.309500]\n",
      "485 [D loss: 0.309334]\n",
      "486 [D loss: 0.312731]\n",
      "487 [D loss: 0.303948]\n",
      "488 [D loss: 0.311016]\n",
      "489 [D loss: 0.310140]\n",
      "490 [D loss: 0.309051]\n",
      "491 [D loss: 0.311039]\n",
      "492 [D loss: 0.306068]\n",
      "493 [D loss: 0.306218]\n",
      "494 [D loss: 0.304343]\n",
      "495 [D loss: 0.362002]\n",
      "496 [D loss: 0.306697]\n",
      "497 [D loss: 0.312701]\n",
      "498 [D loss: 0.310199]\n",
      "499 [D loss: 0.304710]\n",
      "500 [D loss: 0.306187]\n",
      "501 [D loss: 0.300241]\n",
      "502 [D loss: 0.309137]\n",
      "503 [D loss: 0.304912]\n",
      "504 [D loss: 0.300799]\n",
      "505 [D loss: 0.305461]\n",
      "506 [D loss: 0.304207]\n",
      "507 [D loss: 0.334947]\n",
      "508 [D loss: 0.306436]\n",
      "509 [D loss: 0.303619]\n",
      "510 [D loss: 0.309942]\n",
      "511 [D loss: 0.296429]\n",
      "512 [D loss: 0.306604]\n",
      "513 [D loss: 0.301567]\n",
      "514 [D loss: 0.301325]\n",
      "515 [D loss: 0.313977]\n",
      "516 [D loss: 0.302570]\n",
      "517 [D loss: 0.300231]\n",
      "518 [D loss: 0.304556]\n",
      "519 [D loss: 0.305196]\n",
      "520 [D loss: 0.300605]\n",
      "521 [D loss: 0.296907]\n",
      "522 [D loss: 0.301934]\n",
      "523 [D loss: 0.300907]\n",
      "524 [D loss: 0.302738]\n",
      "525 [D loss: 0.297450]\n",
      "526 [D loss: 0.312237]\n",
      "527 [D loss: 0.297049]\n",
      "528 [D loss: 0.303254]\n",
      "529 [D loss: 0.302939]\n",
      "530 [D loss: 0.295506]\n",
      "531 [D loss: 0.295491]\n",
      "532 [D loss: 0.301668]\n",
      "533 [D loss: 0.303541]\n",
      "534 [D loss: 0.302020]\n",
      "535 [D loss: 0.310994]\n",
      "536 [D loss: 0.306442]\n",
      "537 [D loss: 0.312533]\n",
      "538 [D loss: 0.309738]\n",
      "539 [D loss: 0.303639]\n",
      "540 [D loss: 0.302257]\n",
      "541 [D loss: 0.303829]\n",
      "542 [D loss: 0.300595]\n",
      "543 [D loss: 0.299258]\n",
      "544 [D loss: 0.298801]\n",
      "545 [D loss: 0.294082]\n",
      "546 [D loss: 0.293293]\n",
      "547 [D loss: 0.305073]\n",
      "548 [D loss: 0.303873]\n",
      "549 [D loss: 0.299190]\n",
      "550 [D loss: 0.291757]\n",
      "551 [D loss: 0.300886]\n",
      "552 [D loss: 0.298311]\n",
      "553 [D loss: 0.292955]\n",
      "554 [D loss: 0.298901]\n",
      "555 [D loss: 0.293361]\n",
      "556 [D loss: 0.300492]\n",
      "557 [D loss: 0.295363]\n",
      "558 [D loss: 0.308048]\n",
      "559 [D loss: 0.289878]\n",
      "560 [D loss: 0.299858]\n",
      "561 [D loss: 0.297493]\n",
      "562 [D loss: 0.298803]\n",
      "563 [D loss: 0.295608]\n",
      "564 [D loss: 0.295913]\n",
      "565 [D loss: 0.300311]\n",
      "566 [D loss: 0.297047]\n",
      "567 [D loss: 0.292753]\n",
      "568 [D loss: 0.294668]\n",
      "569 [D loss: 0.298166]\n",
      "570 [D loss: 0.288953]\n",
      "571 [D loss: 0.301025]\n",
      "572 [D loss: 0.297308]\n",
      "573 [D loss: 0.330815]\n",
      "574 [D loss: 0.298016]\n",
      "575 [D loss: 0.290619]\n",
      "576 [D loss: 0.289556]\n",
      "577 [D loss: 0.304420]\n",
      "578 [D loss: 0.292013]\n",
      "579 [D loss: 0.300414]\n",
      "580 [D loss: 0.294823]\n",
      "581 [D loss: 0.299361]\n",
      "582 [D loss: 0.291590]\n",
      "583 [D loss: 0.292815]\n",
      "584 [D loss: 0.294019]\n",
      "585 [D loss: 0.289486]\n",
      "586 [D loss: 0.291028]\n",
      "587 [D loss: 0.291938]\n",
      "588 [D loss: 0.296830]\n",
      "589 [D loss: 0.293950]\n",
      "590 [D loss: 0.293961]\n",
      "591 [D loss: 0.293368]\n",
      "592 [D loss: 0.292509]\n",
      "593 [D loss: 0.292153]\n",
      "594 [D loss: 0.294651]\n",
      "595 [D loss: 0.286088]\n",
      "596 [D loss: 0.287815]\n",
      "597 [D loss: 0.305941]\n",
      "598 [D loss: 0.299809]\n",
      "599 [D loss: 0.297539]\n",
      "600 [D loss: 0.287571]\n",
      "601 [D loss: 0.290283]\n",
      "602 [D loss: 0.290298]\n",
      "603 [D loss: 0.288457]\n",
      "604 [D loss: 0.300902]\n",
      "605 [D loss: 0.299113]\n",
      "606 [D loss: 0.292619]\n",
      "607 [D loss: 0.292569]\n",
      "608 [D loss: 0.290375]\n",
      "609 [D loss: 0.299590]\n",
      "610 [D loss: 0.298617]\n",
      "611 [D loss: 0.293299]\n",
      "612 [D loss: 0.287077]\n",
      "613 [D loss: 0.290639]\n",
      "614 [D loss: 0.288012]\n",
      "615 [D loss: 0.290094]\n",
      "616 [D loss: 0.280293]\n",
      "617 [D loss: 0.292167]\n",
      "618 [D loss: 0.282679]\n",
      "619 [D loss: 0.285678]\n",
      "620 [D loss: 0.288163]\n",
      "621 [D loss: 0.289265]\n",
      "622 [D loss: 0.293090]\n",
      "623 [D loss: 0.295385]\n",
      "624 [D loss: 0.291321]\n",
      "625 [D loss: 0.284533]\n",
      "626 [D loss: 0.290285]\n",
      "627 [D loss: 0.288866]\n",
      "628 [D loss: 0.291996]\n",
      "629 [D loss: 0.293137]\n",
      "630 [D loss: 0.288226]\n",
      "631 [D loss: 0.283879]\n",
      "632 [D loss: 0.290946]\n",
      "633 [D loss: 0.282311]\n",
      "634 [D loss: 0.293857]\n",
      "635 [D loss: 0.305809]\n",
      "636 [D loss: 0.285037]\n",
      "637 [D loss: 0.295734]\n",
      "638 [D loss: 0.279753]\n",
      "639 [D loss: 0.284356]\n",
      "640 [D loss: 0.296256]\n",
      "641 [D loss: 0.288188]\n",
      "642 [D loss: 0.281526]\n",
      "643 [D loss: 0.286668]\n",
      "644 [D loss: 0.288026]\n",
      "645 [D loss: 0.283137]\n",
      "646 [D loss: 0.286090]\n",
      "647 [D loss: 0.288234]\n",
      "648 [D loss: 0.270726]\n",
      "649 [D loss: 0.283531]\n",
      "650 [D loss: 0.284808]\n",
      "651 [D loss: 0.278558]\n",
      "652 [D loss: 0.288294]\n",
      "653 [D loss: 0.278900]\n",
      "654 [D loss: 0.278552]\n",
      "655 [D loss: 0.276348]\n",
      "656 [D loss: 0.282538]\n",
      "657 [D loss: 0.277824]\n",
      "658 [D loss: 0.283787]\n",
      "659 [D loss: 0.278330]\n",
      "660 [D loss: 0.289275]\n",
      "661 [D loss: 0.282231]\n",
      "662 [D loss: 0.277474]\n",
      "663 [D loss: 0.281204]\n",
      "664 [D loss: 0.278668]\n",
      "665 [D loss: 0.278040]\n",
      "666 [D loss: 0.280482]\n",
      "667 [D loss: 0.279006]\n",
      "668 [D loss: 0.277634]\n",
      "669 [D loss: 0.272853]\n",
      "670 [D loss: 0.278695]\n",
      "671 [D loss: 0.274213]\n",
      "672 [D loss: 0.285251]\n",
      "673 [D loss: 0.276969]\n",
      "674 [D loss: 0.273643]\n",
      "675 [D loss: 0.277432]\n",
      "676 [D loss: 0.268848]\n",
      "677 [D loss: 0.276444]\n",
      "678 [D loss: 0.278350]\n",
      "679 [D loss: 0.274716]\n",
      "680 [D loss: 0.277599]\n",
      "681 [D loss: 0.283157]\n",
      "682 [D loss: 0.273405]\n",
      "683 [D loss: 0.277919]\n",
      "684 [D loss: 0.274199]\n",
      "685 [D loss: 0.276909]\n",
      "686 [D loss: 0.282510]\n",
      "687 [D loss: 0.286542]\n",
      "688 [D loss: 0.287929]\n",
      "689 [D loss: 0.276738]\n",
      "690 [D loss: 0.266737]\n",
      "691 [D loss: 0.271832]\n",
      "692 [D loss: 0.278679]\n",
      "693 [D loss: 0.271811]\n",
      "694 [D loss: 0.278961]\n",
      "695 [D loss: 0.271979]\n",
      "696 [D loss: 0.279333]\n",
      "697 [D loss: 0.269732]\n",
      "698 [D loss: 0.276593]\n",
      "699 [D loss: 0.275940]\n",
      "700 [D loss: 0.271982]\n",
      "701 [D loss: 0.269757]\n",
      "702 [D loss: 0.271610]\n",
      "703 [D loss: 0.278605]\n",
      "704 [D loss: 0.290506]\n",
      "705 [D loss: 0.279487]\n",
      "706 [D loss: 0.275226]\n",
      "707 [D loss: 0.276557]\n",
      "708 [D loss: 0.283226]\n",
      "709 [D loss: 0.270270]\n",
      "710 [D loss: 0.283855]\n",
      "711 [D loss: 0.274792]\n",
      "712 [D loss: 0.265457]\n",
      "713 [D loss: 0.274056]\n",
      "714 [D loss: 0.271341]\n",
      "715 [D loss: 0.276766]\n",
      "716 [D loss: 0.271509]\n",
      "717 [D loss: 0.276780]\n",
      "718 [D loss: 0.272243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719 [D loss: 0.275136]\n",
      "720 [D loss: 0.263352]\n",
      "721 [D loss: 0.267721]\n",
      "722 [D loss: 0.278567]\n",
      "723 [D loss: 0.271579]\n",
      "724 [D loss: 0.272953]\n",
      "725 [D loss: 0.285441]\n",
      "726 [D loss: 0.271581]\n",
      "727 [D loss: 0.273598]\n",
      "728 [D loss: 0.276427]\n",
      "729 [D loss: 0.270776]\n",
      "730 [D loss: 0.271626]\n",
      "731 [D loss: 0.268862]\n",
      "732 [D loss: 0.273497]\n",
      "733 [D loss: 0.262384]\n",
      "734 [D loss: 0.265115]\n",
      "735 [D loss: 0.268041]\n",
      "736 [D loss: 0.268923]\n",
      "737 [D loss: 0.263871]\n",
      "738 [D loss: 0.275401]\n",
      "739 [D loss: 0.266247]\n",
      "740 [D loss: 0.262472]\n",
      "741 [D loss: 0.264428]\n",
      "742 [D loss: 0.271376]\n",
      "743 [D loss: 0.261737]\n",
      "744 [D loss: 0.269662]\n",
      "745 [D loss: 0.261394]\n",
      "746 [D loss: 0.269666]\n",
      "747 [D loss: 0.261857]\n",
      "748 [D loss: 0.270687]\n",
      "749 [D loss: 0.257760]\n",
      "750 [D loss: 0.273440]\n",
      "751 [D loss: 0.263558]\n",
      "752 [D loss: 0.269584]\n",
      "753 [D loss: 0.259218]\n",
      "754 [D loss: 0.328958]\n",
      "755 [D loss: 0.266209]\n",
      "756 [D loss: 0.267614]\n",
      "757 [D loss: 0.268450]\n",
      "758 [D loss: 0.265275]\n",
      "759 [D loss: 0.265057]\n",
      "760 [D loss: 0.272539]\n",
      "761 [D loss: 0.270330]\n",
      "762 [D loss: 0.262237]\n",
      "763 [D loss: 0.271109]\n",
      "764 [D loss: 0.274444]\n",
      "765 [D loss: 0.265418]\n",
      "766 [D loss: 0.257717]\n",
      "767 [D loss: 0.261091]\n",
      "768 [D loss: 0.256448]\n",
      "769 [D loss: 0.269668]\n",
      "770 [D loss: 0.262854]\n",
      "771 [D loss: 0.283006]\n",
      "772 [D loss: 0.256659]\n",
      "773 [D loss: 0.259100]\n",
      "774 [D loss: 0.271500]\n",
      "775 [D loss: 0.261450]\n",
      "776 [D loss: 0.260283]\n",
      "777 [D loss: 0.263437]\n",
      "778 [D loss: 0.260612]\n",
      "779 [D loss: 0.266274]\n",
      "780 [D loss: 0.268211]\n",
      "781 [D loss: 0.260062]\n",
      "782 [D loss: 0.268415]\n",
      "783 [D loss: 0.257413]\n",
      "784 [D loss: 0.280163]\n",
      "785 [D loss: 0.259123]\n",
      "786 [D loss: 0.261920]\n",
      "787 [D loss: 0.258639]\n",
      "788 [D loss: 0.261807]\n",
      "789 [D loss: 0.255468]\n",
      "790 [D loss: 0.269819]\n",
      "791 [D loss: 0.259064]\n",
      "792 [D loss: 0.263465]\n",
      "793 [D loss: 0.266673]\n",
      "794 [D loss: 0.255556]\n",
      "795 [D loss: 0.251805]\n",
      "796 [D loss: 0.259297]\n",
      "797 [D loss: 0.262475]\n",
      "798 [D loss: 0.256979]\n",
      "799 [D loss: 0.259669]\n",
      "800 [D loss: 0.260030]\n",
      "801 [D loss: 0.259845]\n",
      "802 [D loss: 0.247437]\n",
      "803 [D loss: 0.265112]\n",
      "804 [D loss: 0.258922]\n",
      "805 [D loss: 0.263293]\n",
      "806 [D loss: 0.258844]\n",
      "807 [D loss: 0.251947]\n",
      "808 [D loss: 0.252667]\n",
      "809 [D loss: 0.258124]\n",
      "810 [D loss: 0.259272]\n",
      "811 [D loss: 0.256876]\n",
      "812 [D loss: 0.253679]\n",
      "813 [D loss: 0.251733]\n",
      "814 [D loss: 0.254105]\n",
      "815 [D loss: 0.264421]\n",
      "816 [D loss: 0.257692]\n",
      "817 [D loss: 0.247890]\n",
      "818 [D loss: 0.261110]\n",
      "819 [D loss: 0.255503]\n",
      "820 [D loss: 0.259785]\n",
      "821 [D loss: 0.264179]\n",
      "822 [D loss: 0.269051]\n",
      "823 [D loss: 0.250590]\n",
      "824 [D loss: 0.247356]\n",
      "825 [D loss: 0.257575]\n",
      "826 [D loss: 0.251873]\n",
      "827 [D loss: 0.267633]\n",
      "828 [D loss: 0.262926]\n",
      "829 [D loss: 0.257170]\n",
      "830 [D loss: 0.248562]\n",
      "831 [D loss: 0.261199]\n",
      "832 [D loss: 0.249464]\n",
      "833 [D loss: 0.248733]\n",
      "834 [D loss: 0.247275]\n",
      "835 [D loss: 0.249372]\n",
      "836 [D loss: 0.253824]\n",
      "837 [D loss: 0.250346]\n",
      "838 [D loss: 0.250097]\n",
      "839 [D loss: 0.252318]\n",
      "840 [D loss: 0.249512]\n",
      "841 [D loss: 0.248302]\n",
      "842 [D loss: 0.249855]\n",
      "843 [D loss: 0.253690]\n",
      "844 [D loss: 0.253158]\n",
      "845 [D loss: 0.261684]\n",
      "846 [D loss: 0.245025]\n",
      "847 [D loss: 0.244983]\n",
      "848 [D loss: 0.244113]\n",
      "849 [D loss: 0.246522]\n",
      "850 [D loss: 0.256716]\n",
      "851 [D loss: 0.247431]\n",
      "852 [D loss: 0.251221]\n",
      "853 [D loss: 0.248929]\n",
      "854 [D loss: 0.270295]\n",
      "855 [D loss: 0.259546]\n",
      "856 [D loss: 0.246122]\n",
      "857 [D loss: 0.267049]\n",
      "858 [D loss: 0.242645]\n",
      "859 [D loss: 0.245303]\n",
      "860 [D loss: 0.252801]\n",
      "861 [D loss: 0.248211]\n",
      "862 [D loss: 0.249622]\n",
      "863 [D loss: 0.250719]\n",
      "864 [D loss: 0.248951]\n",
      "865 [D loss: 0.243740]\n",
      "866 [D loss: 0.243542]\n",
      "867 [D loss: 0.247354]\n",
      "868 [D loss: 0.261439]\n",
      "869 [D loss: 0.249358]\n",
      "870 [D loss: 0.259646]\n",
      "871 [D loss: 0.242530]\n",
      "872 [D loss: 0.242928]\n",
      "873 [D loss: 0.241114]\n",
      "874 [D loss: 0.239355]\n",
      "875 [D loss: 0.242112]\n",
      "876 [D loss: 0.242034]\n",
      "877 [D loss: 0.248368]\n",
      "878 [D loss: 0.244674]\n",
      "879 [D loss: 0.245618]\n",
      "880 [D loss: 0.242292]\n",
      "881 [D loss: 0.245385]\n",
      "882 [D loss: 0.243502]\n",
      "883 [D loss: 0.240055]\n",
      "884 [D loss: 0.240736]\n",
      "885 [D loss: 0.236916]\n",
      "886 [D loss: 0.233512]\n",
      "887 [D loss: 0.240525]\n",
      "888 [D loss: 0.251071]\n",
      "889 [D loss: 0.240876]\n",
      "890 [D loss: 0.241235]\n",
      "891 [D loss: 0.238660]\n",
      "892 [D loss: 0.241552]\n",
      "893 [D loss: 0.242208]\n",
      "894 [D loss: 0.231432]\n",
      "895 [D loss: 0.247464]\n",
      "896 [D loss: 0.242740]\n",
      "897 [D loss: 0.237513]\n",
      "898 [D loss: 0.250102]\n",
      "899 [D loss: 0.242131]\n",
      "900 [D loss: 0.282751]\n",
      "901 [D loss: 0.241280]\n",
      "902 [D loss: 0.238895]\n",
      "903 [D loss: 0.242362]\n",
      "904 [D loss: 0.240049]\n",
      "905 [D loss: 0.242031]\n",
      "906 [D loss: 0.238453]\n",
      "907 [D loss: 0.236970]\n",
      "908 [D loss: 0.245828]\n",
      "909 [D loss: 0.238741]\n",
      "910 [D loss: 0.238198]\n",
      "911 [D loss: 0.233751]\n",
      "912 [D loss: 0.231540]\n",
      "913 [D loss: 0.239334]\n",
      "914 [D loss: 0.245819]\n",
      "915 [D loss: 0.237104]\n",
      "916 [D loss: 0.237002]\n",
      "917 [D loss: 0.234027]\n",
      "918 [D loss: 0.239200]\n",
      "919 [D loss: 0.240875]\n",
      "920 [D loss: 0.233709]\n",
      "921 [D loss: 0.237979]\n",
      "922 [D loss: 0.235465]\n",
      "923 [D loss: 0.245523]\n",
      "924 [D loss: 0.237557]\n",
      "925 [D loss: 0.242371]\n",
      "926 [D loss: 0.231649]\n",
      "927 [D loss: 0.236273]\n",
      "928 [D loss: 0.237947]\n",
      "929 [D loss: 0.237713]\n",
      "930 [D loss: 0.239617]\n",
      "931 [D loss: 0.237799]\n",
      "932 [D loss: 0.227809]\n",
      "933 [D loss: 0.246024]\n",
      "934 [D loss: 0.234226]\n",
      "935 [D loss: 0.237003]\n",
      "936 [D loss: 0.237676]\n",
      "937 [D loss: 0.229258]\n",
      "938 [D loss: 0.236328]\n",
      "939 [D loss: 0.233419]\n",
      "940 [D loss: 0.230358]\n",
      "941 [D loss: 0.232906]\n",
      "942 [D loss: 0.226682]\n",
      "943 [D loss: 0.240037]\n",
      "944 [D loss: 0.236596]\n",
      "945 [D loss: 0.228145]\n",
      "946 [D loss: 0.230187]\n",
      "947 [D loss: 0.226279]\n",
      "948 [D loss: 0.237841]\n",
      "949 [D loss: 0.231758]\n",
      "950 [D loss: 0.241106]\n",
      "951 [D loss: 0.236190]\n",
      "952 [D loss: 0.232282]\n",
      "953 [D loss: 0.248709]\n",
      "954 [D loss: 0.236236]\n",
      "955 [D loss: 0.248926]\n",
      "956 [D loss: 0.238175]\n",
      "957 [D loss: 0.234391]\n",
      "958 [D loss: 0.232425]\n",
      "959 [D loss: 0.234504]\n",
      "960 [D loss: 0.229759]\n",
      "961 [D loss: 0.232547]\n",
      "962 [D loss: 0.238936]\n",
      "963 [D loss: 0.232903]\n",
      "964 [D loss: 0.237715]\n",
      "965 [D loss: 0.224390]\n",
      "966 [D loss: 0.229390]\n",
      "967 [D loss: 0.223868]\n",
      "968 [D loss: 0.232326]\n",
      "969 [D loss: 0.236501]\n",
      "970 [D loss: 0.235719]\n",
      "971 [D loss: 0.229521]\n",
      "972 [D loss: 0.224607]\n",
      "973 [D loss: 0.226051]\n",
      "974 [D loss: 0.230436]\n",
      "975 [D loss: 0.226723]\n",
      "976 [D loss: 0.223773]\n",
      "977 [D loss: 0.241639]\n",
      "978 [D loss: 0.232225]\n",
      "979 [D loss: 0.221128]\n",
      "980 [D loss: 0.232128]\n",
      "981 [D loss: 0.231955]\n",
      "982 [D loss: 0.236555]\n",
      "983 [D loss: 0.236999]\n",
      "984 [D loss: 0.231217]\n",
      "985 [D loss: 0.229596]\n",
      "986 [D loss: 0.224112]\n",
      "987 [D loss: 0.231544]\n",
      "988 [D loss: 0.232512]\n",
      "989 [D loss: 0.281934]\n",
      "990 [D loss: 0.226617]\n",
      "991 [D loss: 0.225049]\n",
      "992 [D loss: 0.238163]\n",
      "993 [D loss: 0.224770]\n",
      "994 [D loss: 0.239067]\n",
      "995 [D loss: 0.225611]\n",
      "996 [D loss: 0.223194]\n",
      "997 [D loss: 0.225834]\n",
      "998 [D loss: 0.228616]\n",
      "999 [D loss: 0.235981]\n",
      "1000 [D loss: 0.227072]\n",
      "1001 [D loss: 0.227527]\n",
      "1002 [D loss: 0.226290]\n",
      "1003 [D loss: 0.224579]\n",
      "1004 [D loss: 0.225823]\n",
      "1005 [D loss: 0.224467]\n",
      "1006 [D loss: 0.229360]\n",
      "1007 [D loss: 0.224461]\n",
      "1008 [D loss: 0.231539]\n",
      "1009 [D loss: 0.225657]\n",
      "1010 [D loss: 0.233871]\n",
      "1011 [D loss: 0.217139]\n",
      "1012 [D loss: 0.229466]\n",
      "1013 [D loss: 0.227443]\n",
      "1014 [D loss: 0.231094]\n",
      "1015 [D loss: 0.224037]\n",
      "1016 [D loss: 0.224344]\n",
      "1017 [D loss: 0.219046]\n",
      "1018 [D loss: 0.222291]\n",
      "1019 [D loss: 0.222395]\n",
      "1020 [D loss: 0.218897]\n",
      "1021 [D loss: 0.222638]\n",
      "1022 [D loss: 0.230843]\n",
      "1023 [D loss: 0.225779]\n",
      "1024 [D loss: 0.219890]\n",
      "1025 [D loss: 0.236913]\n",
      "1026 [D loss: 0.218326]\n",
      "1027 [D loss: 0.226042]\n",
      "1028 [D loss: 0.228769]\n",
      "1029 [D loss: 0.228839]\n",
      "1030 [D loss: 0.222930]\n",
      "1031 [D loss: 0.223467]\n",
      "1032 [D loss: 0.224051]\n",
      "1033 [D loss: 0.230328]\n",
      "1034 [D loss: 0.216714]\n",
      "1035 [D loss: 0.221598]\n",
      "1036 [D loss: 0.224906]\n",
      "1037 [D loss: 0.222865]\n",
      "1038 [D loss: 0.215321]\n",
      "1039 [D loss: 0.218274]\n",
      "1040 [D loss: 0.213107]\n",
      "1041 [D loss: 0.216347]\n",
      "1042 [D loss: 0.231358]\n",
      "1043 [D loss: 0.218236]\n",
      "1044 [D loss: 0.218332]\n",
      "1045 [D loss: 0.216368]\n",
      "1046 [D loss: 0.217988]\n",
      "1047 [D loss: 0.215708]\n",
      "1048 [D loss: 0.296404]\n",
      "1049 [D loss: 0.213708]\n",
      "1050 [D loss: 0.217478]\n",
      "1051 [D loss: 0.215741]\n",
      "1052 [D loss: 0.223289]\n",
      "1053 [D loss: 0.219096]\n",
      "1054 [D loss: 0.215949]\n",
      "1055 [D loss: 0.217237]\n",
      "1056 [D loss: 0.214006]\n",
      "1057 [D loss: 0.221821]\n",
      "1058 [D loss: 0.213330]\n",
      "1059 [D loss: 0.207738]\n",
      "1060 [D loss: 0.225753]\n",
      "1061 [D loss: 0.214630]\n",
      "1062 [D loss: 0.224422]\n",
      "1063 [D loss: 0.212897]\n",
      "1064 [D loss: 0.213913]\n",
      "1065 [D loss: 0.258405]\n",
      "1066 [D loss: 0.221575]\n",
      "1067 [D loss: 0.216120]\n",
      "1068 [D loss: 0.219232]\n",
      "1069 [D loss: 0.218974]\n",
      "1070 [D loss: 0.214063]\n",
      "1071 [D loss: 0.228228]\n",
      "1072 [D loss: 0.213559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1073 [D loss: 0.223276]\n",
      "1074 [D loss: 0.215080]\n",
      "1075 [D loss: 0.216989]\n",
      "1076 [D loss: 0.216015]\n",
      "1077 [D loss: 0.216647]\n",
      "1078 [D loss: 0.247748]\n",
      "1079 [D loss: 0.220541]\n",
      "1080 [D loss: 0.214518]\n",
      "1081 [D loss: 0.215504]\n",
      "1082 [D loss: 0.220075]\n",
      "1083 [D loss: 0.209746]\n",
      "1084 [D loss: 0.218395]\n",
      "1085 [D loss: 0.209641]\n",
      "1086 [D loss: 0.211919]\n",
      "1087 [D loss: 0.211294]\n",
      "1088 [D loss: 0.207582]\n",
      "1089 [D loss: 0.214971]\n",
      "1090 [D loss: 0.208086]\n",
      "1091 [D loss: 0.222644]\n",
      "1092 [D loss: 0.215646]\n",
      "1093 [D loss: 0.206896]\n",
      "1094 [D loss: 0.211418]\n",
      "1095 [D loss: 0.209477]\n",
      "1096 [D loss: 0.209207]\n",
      "1097 [D loss: 0.212992]\n",
      "1098 [D loss: 0.211073]\n",
      "1099 [D loss: 0.213281]\n",
      "1100 [D loss: 0.215260]\n",
      "1101 [D loss: 0.209953]\n",
      "1102 [D loss: 0.205591]\n",
      "1103 [D loss: 0.215391]\n",
      "1104 [D loss: 0.205431]\n",
      "1105 [D loss: 0.213849]\n",
      "1106 [D loss: 0.202185]\n",
      "1107 [D loss: 0.205390]\n",
      "1108 [D loss: 0.213414]\n",
      "1109 [D loss: 0.215216]\n",
      "1110 [D loss: 0.208652]\n",
      "1111 [D loss: 0.211170]\n",
      "1112 [D loss: 0.219396]\n",
      "1113 [D loss: 0.203436]\n",
      "1114 [D loss: 0.201764]\n",
      "1115 [D loss: 0.208669]\n",
      "1116 [D loss: 0.211465]\n",
      "1117 [D loss: 0.207491]\n",
      "1118 [D loss: 0.215624]\n",
      "1119 [D loss: 0.203056]\n",
      "1120 [D loss: 0.205581]\n",
      "1121 [D loss: 0.210884]\n",
      "1122 [D loss: 0.213577]\n",
      "1123 [D loss: 0.212104]\n",
      "1124 [D loss: 0.212434]\n",
      "1125 [D loss: 0.220184]\n",
      "1126 [D loss: 0.217062]\n",
      "1127 [D loss: 0.206335]\n",
      "1128 [D loss: 0.210687]\n",
      "1129 [D loss: 0.210728]\n",
      "1130 [D loss: 0.209935]\n",
      "1131 [D loss: 0.210295]\n",
      "1132 [D loss: 0.205854]\n",
      "1133 [D loss: 0.207320]\n",
      "1134 [D loss: 0.206266]\n",
      "1135 [D loss: 0.207901]\n",
      "1136 [D loss: 0.199264]\n",
      "1137 [D loss: 0.207833]\n",
      "1138 [D loss: 0.202289]\n",
      "1139 [D loss: 0.198120]\n",
      "1140 [D loss: 0.201029]\n",
      "1141 [D loss: 0.206860]\n",
      "1142 [D loss: 0.201766]\n",
      "1143 [D loss: 0.204089]\n",
      "1144 [D loss: 0.200623]\n",
      "1145 [D loss: 0.202515]\n",
      "1146 [D loss: 0.211156]\n",
      "1147 [D loss: 0.209084]\n",
      "1148 [D loss: 0.204080]\n",
      "1149 [D loss: 0.205913]\n",
      "1150 [D loss: 0.198575]\n",
      "1151 [D loss: 0.200802]\n",
      "1152 [D loss: 0.203021]\n",
      "1153 [D loss: 0.203180]\n",
      "1154 [D loss: 0.200677]\n",
      "1155 [D loss: 0.205796]\n",
      "1156 [D loss: 0.202519]\n",
      "1157 [D loss: 0.201101]\n",
      "1158 [D loss: 0.196926]\n",
      "1159 [D loss: 0.194694]\n",
      "1160 [D loss: 0.199049]\n",
      "1161 [D loss: 0.202536]\n",
      "1162 [D loss: 0.201454]\n",
      "1163 [D loss: 0.206523]\n",
      "1164 [D loss: 0.208090]\n",
      "1165 [D loss: 0.199356]\n",
      "1166 [D loss: 0.204959]\n",
      "1167 [D loss: 0.198915]\n",
      "1168 [D loss: 0.196710]\n",
      "1169 [D loss: 0.209590]\n",
      "1170 [D loss: 0.206699]\n",
      "1171 [D loss: 0.196253]\n",
      "1172 [D loss: 0.203843]\n",
      "1173 [D loss: 0.220374]\n",
      "1174 [D loss: 0.197019]\n",
      "1175 [D loss: 0.204032]\n",
      "1176 [D loss: 0.197971]\n",
      "1177 [D loss: 0.205442]\n",
      "1178 [D loss: 0.199803]\n",
      "1179 [D loss: 0.203419]\n",
      "1180 [D loss: 0.202053]\n",
      "1181 [D loss: 0.201544]\n",
      "1182 [D loss: 0.212620]\n",
      "1183 [D loss: 0.205899]\n",
      "1184 [D loss: 0.203351]\n",
      "1185 [D loss: 0.207620]\n",
      "1186 [D loss: 0.193163]\n",
      "1187 [D loss: 0.194138]\n",
      "1188 [D loss: 0.197361]\n",
      "1189 [D loss: 0.190653]\n",
      "1190 [D loss: 0.202396]\n",
      "1191 [D loss: 0.194829]\n",
      "1192 [D loss: 0.198407]\n",
      "1193 [D loss: 0.191929]\n",
      "1194 [D loss: 0.196424]\n",
      "1195 [D loss: 0.197761]\n",
      "1196 [D loss: 0.196656]\n",
      "1197 [D loss: 0.211903]\n",
      "1198 [D loss: 0.196655]\n",
      "1199 [D loss: 0.211497]\n",
      "1200 [D loss: 0.202272]\n",
      "1201 [D loss: 0.205867]\n",
      "1202 [D loss: 0.204003]\n",
      "1203 [D loss: 0.199005]\n",
      "1204 [D loss: 0.198198]\n",
      "1205 [D loss: 0.202000]\n",
      "1206 [D loss: 0.197371]\n",
      "1207 [D loss: 0.192659]\n",
      "1208 [D loss: 0.201660]\n",
      "1209 [D loss: 0.195881]\n",
      "1210 [D loss: 0.192629]\n",
      "1211 [D loss: 0.196630]\n",
      "1212 [D loss: 0.186802]\n",
      "1213 [D loss: 0.196866]\n",
      "1214 [D loss: 0.198642]\n",
      "1215 [D loss: 0.241556]\n",
      "1216 [D loss: 0.193696]\n",
      "1217 [D loss: 0.192683]\n",
      "1218 [D loss: 0.247661]\n",
      "1219 [D loss: 0.196465]\n",
      "1220 [D loss: 0.191215]\n",
      "1221 [D loss: 0.197676]\n",
      "1222 [D loss: 0.205282]\n",
      "1223 [D loss: 0.204379]\n",
      "1224 [D loss: 0.204796]\n",
      "1225 [D loss: 0.192343]\n",
      "1226 [D loss: 0.197957]\n",
      "1227 [D loss: 0.194679]\n",
      "1228 [D loss: 0.193850]\n",
      "1229 [D loss: 0.193032]\n",
      "1230 [D loss: 0.198282]\n",
      "1231 [D loss: 0.190030]\n",
      "1232 [D loss: 0.190092]\n",
      "1233 [D loss: 0.193085]\n",
      "1234 [D loss: 0.189318]\n",
      "1235 [D loss: 0.191991]\n",
      "1236 [D loss: 0.189096]\n",
      "1237 [D loss: 0.202072]\n",
      "1238 [D loss: 0.190351]\n",
      "1239 [D loss: 0.195233]\n",
      "1240 [D loss: 0.200251]\n",
      "1241 [D loss: 0.189205]\n",
      "1242 [D loss: 0.191978]\n",
      "1243 [D loss: 0.192977]\n",
      "1244 [D loss: 0.186839]\n",
      "1245 [D loss: 0.200819]\n",
      "1246 [D loss: 0.194838]\n",
      "1247 [D loss: 0.187672]\n",
      "1248 [D loss: 0.188770]\n",
      "1249 [D loss: 0.196899]\n",
      "1250 [D loss: 0.195879]\n",
      "1251 [D loss: 0.190891]\n",
      "1252 [D loss: 0.191128]\n",
      "1253 [D loss: 0.192458]\n",
      "1254 [D loss: 0.192851]\n",
      "1255 [D loss: 0.186952]\n",
      "1256 [D loss: 0.191032]\n",
      "1257 [D loss: 0.186892]\n",
      "1258 [D loss: 0.189260]\n",
      "1259 [D loss: 0.187729]\n",
      "1260 [D loss: 0.198554]\n",
      "1261 [D loss: 0.202839]\n",
      "1262 [D loss: 0.188309]\n",
      "1263 [D loss: 0.200966]\n",
      "1264 [D loss: 0.194141]\n",
      "1265 [D loss: 0.192875]\n",
      "1266 [D loss: 0.194277]\n",
      "1267 [D loss: 0.190091]\n",
      "1268 [D loss: 0.184328]\n",
      "1269 [D loss: 0.183772]\n",
      "1270 [D loss: 0.193340]\n",
      "1271 [D loss: 0.201139]\n",
      "1272 [D loss: 0.194776]\n",
      "1273 [D loss: 0.184860]\n",
      "1274 [D loss: 0.179680]\n",
      "1275 [D loss: 0.192057]\n",
      "1276 [D loss: 0.189330]\n",
      "1277 [D loss: 0.186744]\n",
      "1278 [D loss: 0.183365]\n",
      "1279 [D loss: 0.179671]\n",
      "1280 [D loss: 0.180211]\n",
      "1281 [D loss: 0.182415]\n",
      "1282 [D loss: 0.182585]\n",
      "1283 [D loss: 0.183915]\n",
      "1284 [D loss: 0.188941]\n",
      "1285 [D loss: 0.178374]\n",
      "1286 [D loss: 0.191347]\n",
      "1287 [D loss: 0.183201]\n",
      "1288 [D loss: 0.189522]\n",
      "1289 [D loss: 0.194470]\n",
      "1290 [D loss: 0.183882]\n",
      "1291 [D loss: 0.189432]\n",
      "1292 [D loss: 0.214348]\n",
      "1293 [D loss: 0.181625]\n",
      "1294 [D loss: 0.185272]\n",
      "1295 [D loss: 0.190369]\n",
      "1296 [D loss: 0.187524]\n",
      "1297 [D loss: 0.185295]\n",
      "1298 [D loss: 0.196746]\n",
      "1299 [D loss: 0.186432]\n",
      "1300 [D loss: 0.184034]\n",
      "1301 [D loss: 0.184110]\n",
      "1302 [D loss: 0.183419]\n",
      "1303 [D loss: 0.178471]\n",
      "1304 [D loss: 0.191256]\n",
      "1305 [D loss: 0.186378]\n",
      "1306 [D loss: 0.181773]\n",
      "1307 [D loss: 0.189086]\n",
      "1308 [D loss: 0.192066]\n",
      "1309 [D loss: 0.179237]\n",
      "1310 [D loss: 0.184008]\n",
      "1311 [D loss: 0.183207]\n",
      "1312 [D loss: 0.184583]\n",
      "1313 [D loss: 0.182406]\n",
      "1314 [D loss: 0.190526]\n",
      "1315 [D loss: 0.179435]\n",
      "1316 [D loss: 0.179632]\n",
      "1317 [D loss: 0.198578]\n",
      "1318 [D loss: 0.180306]\n",
      "1319 [D loss: 0.183789]\n",
      "1320 [D loss: 0.175721]\n",
      "1321 [D loss: 0.195146]\n",
      "1322 [D loss: 0.208386]\n",
      "1323 [D loss: 0.174433]\n",
      "1324 [D loss: 0.178923]\n",
      "1325 [D loss: 0.183765]\n",
      "1326 [D loss: 0.206504]\n",
      "1327 [D loss: 0.180875]\n",
      "1328 [D loss: 0.187502]\n",
      "1329 [D loss: 0.177390]\n",
      "1330 [D loss: 0.182952]\n",
      "1331 [D loss: 0.181972]\n",
      "1332 [D loss: 0.182079]\n",
      "1333 [D loss: 0.191753]\n",
      "1334 [D loss: 0.182841]\n",
      "1335 [D loss: 0.183713]\n",
      "1336 [D loss: 0.201213]\n",
      "1337 [D loss: 0.178884]\n",
      "1338 [D loss: 0.181064]\n",
      "1339 [D loss: 0.178151]\n",
      "1340 [D loss: 0.183185]\n",
      "1341 [D loss: 0.186040]\n",
      "1342 [D loss: 0.180224]\n",
      "1343 [D loss: 0.183342]\n",
      "1344 [D loss: 0.181246]\n",
      "1345 [D loss: 0.176032]\n",
      "1346 [D loss: 0.179859]\n",
      "1347 [D loss: 0.173606]\n",
      "1348 [D loss: 0.179849]\n",
      "1349 [D loss: 0.176625]\n",
      "1350 [D loss: 0.175984]\n",
      "1351 [D loss: 0.179934]\n",
      "1352 [D loss: 0.184985]\n",
      "1353 [D loss: 0.180356]\n",
      "1354 [D loss: 0.181626]\n",
      "1355 [D loss: 0.176710]\n",
      "1356 [D loss: 0.178364]\n",
      "1357 [D loss: 0.175455]\n",
      "1358 [D loss: 0.171555]\n",
      "1359 [D loss: 0.178200]\n",
      "1360 [D loss: 0.173380]\n",
      "1361 [D loss: 0.181728]\n",
      "1362 [D loss: 0.192039]\n",
      "1363 [D loss: 0.181283]\n",
      "1364 [D loss: 0.168795]\n",
      "1365 [D loss: 0.178581]\n",
      "1366 [D loss: 0.178447]\n",
      "1367 [D loss: 0.180136]\n",
      "1368 [D loss: 0.176512]\n",
      "1369 [D loss: 0.172590]\n",
      "1370 [D loss: 0.177076]\n",
      "1371 [D loss: 0.177833]\n",
      "1372 [D loss: 0.170989]\n",
      "1373 [D loss: 0.177634]\n",
      "1374 [D loss: 0.182616]\n",
      "1375 [D loss: 0.174553]\n",
      "1376 [D loss: 0.172317]\n",
      "1377 [D loss: 0.175232]\n",
      "1378 [D loss: 0.175211]\n",
      "1379 [D loss: 0.181387]\n",
      "1380 [D loss: 0.174125]\n",
      "1381 [D loss: 0.170151]\n",
      "1382 [D loss: 0.174085]\n",
      "1383 [D loss: 0.184609]\n",
      "1384 [D loss: 0.179258]\n",
      "1385 [D loss: 0.176972]\n",
      "1386 [D loss: 0.164643]\n",
      "1387 [D loss: 0.181981]\n",
      "1388 [D loss: 0.180476]\n",
      "1389 [D loss: 0.180488]\n",
      "1390 [D loss: 0.176878]\n",
      "1391 [D loss: 0.175899]\n",
      "1392 [D loss: 0.172430]\n",
      "1393 [D loss: 0.171615]\n",
      "1394 [D loss: 0.169482]\n",
      "1395 [D loss: 0.174878]\n",
      "1396 [D loss: 0.173550]\n",
      "1397 [D loss: 0.175418]\n",
      "1398 [D loss: 0.171957]\n",
      "1399 [D loss: 0.169131]\n",
      "1400 [D loss: 0.175905]\n",
      "1401 [D loss: 0.174942]\n",
      "1402 [D loss: 0.170982]\n",
      "1403 [D loss: 0.176339]\n",
      "1404 [D loss: 0.172827]\n",
      "1405 [D loss: 0.166497]\n",
      "1406 [D loss: 0.170499]\n",
      "1407 [D loss: 0.172975]\n",
      "1408 [D loss: 0.179649]\n",
      "1409 [D loss: 0.175056]\n",
      "1410 [D loss: 0.168189]\n",
      "1411 [D loss: 0.177075]\n",
      "1412 [D loss: 0.174857]\n",
      "1413 [D loss: 0.172759]\n",
      "1414 [D loss: 0.162396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1415 [D loss: 0.171902]\n",
      "1416 [D loss: 0.182527]\n",
      "1417 [D loss: 0.223097]\n",
      "1418 [D loss: 0.166039]\n",
      "1419 [D loss: 0.164080]\n",
      "1420 [D loss: 0.182927]\n",
      "1421 [D loss: 0.178711]\n",
      "1422 [D loss: 0.172365]\n",
      "1423 [D loss: 0.167417]\n",
      "1424 [D loss: 0.170864]\n",
      "1425 [D loss: 0.164128]\n",
      "1426 [D loss: 0.177542]\n",
      "1427 [D loss: 0.174566]\n",
      "1428 [D loss: 0.171975]\n",
      "1429 [D loss: 0.178469]\n",
      "1430 [D loss: 0.170564]\n",
      "1431 [D loss: 0.175061]\n",
      "1432 [D loss: 0.173016]\n",
      "1433 [D loss: 0.165225]\n",
      "1434 [D loss: 0.172160]\n",
      "1435 [D loss: 0.179535]\n",
      "1436 [D loss: 0.163909]\n",
      "1437 [D loss: 0.165031]\n",
      "1438 [D loss: 0.173212]\n",
      "1439 [D loss: 0.164629]\n",
      "1440 [D loss: 0.168311]\n",
      "1441 [D loss: 0.174264]\n",
      "1442 [D loss: 0.166996]\n",
      "1443 [D loss: 0.171023]\n",
      "1444 [D loss: 0.169427]\n",
      "1445 [D loss: 0.164540]\n",
      "1446 [D loss: 0.177205]\n",
      "1447 [D loss: 0.160845]\n",
      "1448 [D loss: 0.168069]\n",
      "1449 [D loss: 0.165348]\n",
      "1450 [D loss: 0.172072]\n",
      "1451 [D loss: 0.171646]\n",
      "1452 [D loss: 0.176803]\n",
      "1453 [D loss: 0.162941]\n",
      "1454 [D loss: 0.171473]\n",
      "1455 [D loss: 0.159878]\n",
      "1456 [D loss: 0.171805]\n",
      "1457 [D loss: 0.170236]\n",
      "1458 [D loss: 0.164460]\n",
      "1459 [D loss: 0.175248]\n",
      "1460 [D loss: 0.166918]\n",
      "1461 [D loss: 0.162164]\n",
      "1462 [D loss: 0.161666]\n",
      "1463 [D loss: 0.164317]\n",
      "1464 [D loss: 0.166311]\n",
      "1465 [D loss: 0.169097]\n",
      "1466 [D loss: 0.162468]\n",
      "1467 [D loss: 0.166293]\n",
      "1468 [D loss: 0.163054]\n",
      "1469 [D loss: 0.166574]\n",
      "1470 [D loss: 0.165194]\n",
      "1471 [D loss: 0.165444]\n",
      "1472 [D loss: 0.160270]\n",
      "1473 [D loss: 0.158416]\n",
      "1474 [D loss: 0.161518]\n",
      "1475 [D loss: 0.162094]\n",
      "1476 [D loss: 0.162705]\n",
      "1477 [D loss: 0.165663]\n",
      "1478 [D loss: 0.160863]\n",
      "1479 [D loss: 0.159160]\n",
      "1480 [D loss: 0.160245]\n",
      "1481 [D loss: 0.159616]\n",
      "1482 [D loss: 0.165684]\n",
      "1483 [D loss: 0.157782]\n",
      "1484 [D loss: 0.158534]\n",
      "1485 [D loss: 0.152838]\n",
      "1486 [D loss: 0.158125]\n",
      "1487 [D loss: 0.162035]\n",
      "1488 [D loss: 0.168605]\n",
      "1489 [D loss: 0.192407]\n",
      "1490 [D loss: 0.162578]\n",
      "1491 [D loss: 0.171163]\n",
      "1492 [D loss: 0.155562]\n",
      "1493 [D loss: 0.174356]\n",
      "1494 [D loss: 0.176155]\n",
      "1495 [D loss: 0.160233]\n",
      "1496 [D loss: 0.165484]\n",
      "1497 [D loss: 0.173305]\n",
      "1498 [D loss: 0.164883]\n",
      "1499 [D loss: 0.161963]\n",
      "1500 [D loss: 0.272359]\n",
      "1501 [D loss: 0.165808]\n",
      "1502 [D loss: 0.162234]\n",
      "1503 [D loss: 0.165402]\n",
      "1504 [D loss: 0.163635]\n",
      "1505 [D loss: 0.165427]\n",
      "1506 [D loss: 0.170087]\n",
      "1507 [D loss: 0.183892]\n",
      "1508 [D loss: 0.173355]\n",
      "1509 [D loss: 0.167664]\n",
      "1510 [D loss: 0.164487]\n",
      "1511 [D loss: 0.154984]\n",
      "1512 [D loss: 0.163075]\n",
      "1513 [D loss: 0.167978]\n",
      "1514 [D loss: 0.157185]\n",
      "1515 [D loss: 0.160932]\n",
      "1516 [D loss: 0.183902]\n",
      "1517 [D loss: 0.161830]\n",
      "1518 [D loss: 0.157745]\n",
      "1519 [D loss: 0.155529]\n",
      "1520 [D loss: 0.157922]\n",
      "1521 [D loss: 0.158622]\n",
      "1522 [D loss: 0.154920]\n",
      "1523 [D loss: 0.157824]\n",
      "1524 [D loss: 0.167381]\n",
      "1525 [D loss: 0.165242]\n",
      "1526 [D loss: 0.158941]\n",
      "1527 [D loss: 0.167933]\n",
      "1528 [D loss: 0.171357]\n",
      "1529 [D loss: 0.156443]\n",
      "1530 [D loss: 0.157965]\n",
      "1531 [D loss: 0.218570]\n",
      "1532 [D loss: 0.162622]\n",
      "1533 [D loss: 0.164026]\n",
      "1534 [D loss: 0.182315]\n",
      "1535 [D loss: 0.153595]\n",
      "1536 [D loss: 0.169165]\n",
      "1537 [D loss: 0.158160]\n",
      "1538 [D loss: 0.163510]\n",
      "1539 [D loss: 0.155867]\n",
      "1540 [D loss: 0.161671]\n",
      "1541 [D loss: 0.153772]\n",
      "1542 [D loss: 0.153930]\n",
      "1543 [D loss: 0.151551]\n",
      "1544 [D loss: 0.160488]\n",
      "1545 [D loss: 0.162640]\n",
      "1546 [D loss: 0.154171]\n",
      "1547 [D loss: 0.157577]\n",
      "1548 [D loss: 0.161695]\n",
      "1549 [D loss: 0.160628]\n",
      "1550 [D loss: 0.164107]\n",
      "1551 [D loss: 0.161082]\n",
      "1552 [D loss: 0.157444]\n",
      "1553 [D loss: 0.154192]\n",
      "1554 [D loss: 0.152489]\n",
      "1555 [D loss: 0.153656]\n",
      "1556 [D loss: 0.148925]\n",
      "1557 [D loss: 0.147235]\n",
      "1558 [D loss: 0.153607]\n",
      "1559 [D loss: 0.164746]\n",
      "1560 [D loss: 0.178529]\n",
      "1561 [D loss: 0.151608]\n",
      "1562 [D loss: 0.154765]\n",
      "1563 [D loss: 0.156458]\n",
      "1564 [D loss: 0.167551]\n",
      "1565 [D loss: 0.152583]\n",
      "1566 [D loss: 0.153275]\n",
      "1567 [D loss: 0.153478]\n",
      "1568 [D loss: 0.160470]\n",
      "1569 [D loss: 0.161260]\n",
      "1570 [D loss: 0.154219]\n",
      "1571 [D loss: 0.164346]\n",
      "1572 [D loss: 0.153874]\n",
      "1573 [D loss: 0.157488]\n",
      "1574 [D loss: 0.151000]\n",
      "1575 [D loss: 0.157505]\n",
      "1576 [D loss: 0.149721]\n",
      "1577 [D loss: 0.150074]\n",
      "1578 [D loss: 0.153113]\n",
      "1579 [D loss: 0.150919]\n",
      "1580 [D loss: 0.164836]\n",
      "1581 [D loss: 0.147971]\n",
      "1582 [D loss: 0.148652]\n",
      "1583 [D loss: 0.155646]\n",
      "1584 [D loss: 0.150485]\n",
      "1585 [D loss: 0.154399]\n",
      "1586 [D loss: 0.162180]\n",
      "1587 [D loss: 0.147399]\n",
      "1588 [D loss: 0.149146]\n",
      "1589 [D loss: 0.152134]\n",
      "1590 [D loss: 0.148965]\n",
      "1591 [D loss: 0.145366]\n",
      "1592 [D loss: 0.152582]\n",
      "1593 [D loss: 0.151504]\n",
      "1594 [D loss: 0.149385]\n",
      "1595 [D loss: 0.152417]\n",
      "1596 [D loss: 0.160611]\n",
      "1597 [D loss: 0.151610]\n",
      "1598 [D loss: 0.157155]\n",
      "1599 [D loss: 0.144454]\n",
      "1600 [D loss: 0.150625]\n",
      "1601 [D loss: 0.160863]\n",
      "1602 [D loss: 0.151656]\n",
      "1603 [D loss: 0.153395]\n",
      "1604 [D loss: 0.147739]\n",
      "1605 [D loss: 0.146850]\n",
      "1606 [D loss: 0.156822]\n",
      "1607 [D loss: 0.153197]\n",
      "1608 [D loss: 0.154254]\n",
      "1609 [D loss: 0.152855]\n",
      "1610 [D loss: 0.157908]\n",
      "1611 [D loss: 0.148661]\n",
      "1612 [D loss: 0.147288]\n",
      "1613 [D loss: 0.167767]\n",
      "1614 [D loss: 0.146548]\n",
      "1615 [D loss: 0.151096]\n",
      "1616 [D loss: 0.153756]\n",
      "1617 [D loss: 0.155128]\n",
      "1618 [D loss: 0.151281]\n",
      "1619 [D loss: 0.147957]\n",
      "1620 [D loss: 0.152220]\n",
      "1621 [D loss: 0.161725]\n",
      "1622 [D loss: 0.149079]\n",
      "1623 [D loss: 0.153538]\n",
      "1624 [D loss: 0.145215]\n",
      "1625 [D loss: 0.143649]\n",
      "1626 [D loss: 0.156707]\n",
      "1627 [D loss: 0.148674]\n",
      "1628 [D loss: 0.145522]\n",
      "1629 [D loss: 0.149066]\n",
      "1630 [D loss: 0.154172]\n",
      "1631 [D loss: 0.144370]\n",
      "1632 [D loss: 0.141773]\n",
      "1633 [D loss: 0.154597]\n",
      "1634 [D loss: 0.144662]\n",
      "1635 [D loss: 0.148614]\n",
      "1636 [D loss: 0.149426]\n",
      "1637 [D loss: 0.145116]\n",
      "1638 [D loss: 0.147919]\n",
      "1639 [D loss: 0.147135]\n",
      "1640 [D loss: 0.151705]\n",
      "1641 [D loss: 0.155393]\n",
      "1642 [D loss: 0.151505]\n",
      "1643 [D loss: 0.150872]\n",
      "1644 [D loss: 0.145835]\n",
      "1645 [D loss: 0.151975]\n",
      "1646 [D loss: 0.143469]\n",
      "1647 [D loss: 0.149579]\n",
      "1648 [D loss: 0.148549]\n",
      "1649 [D loss: 0.148398]\n",
      "1650 [D loss: 0.148768]\n",
      "1651 [D loss: 0.137949]\n",
      "1652 [D loss: 0.158482]\n",
      "1653 [D loss: 0.143503]\n",
      "1654 [D loss: 0.146204]\n",
      "1655 [D loss: 0.151826]\n",
      "1656 [D loss: 0.138086]\n",
      "1657 [D loss: 0.145162]\n",
      "1658 [D loss: 0.152026]\n",
      "1659 [D loss: 0.139416]\n",
      "1660 [D loss: 0.145842]\n",
      "1661 [D loss: 0.153204]\n",
      "1662 [D loss: 0.151362]\n",
      "1663 [D loss: 0.136643]\n",
      "1664 [D loss: 0.146886]\n",
      "1665 [D loss: 0.145559]\n",
      "1666 [D loss: 0.144215]\n",
      "1667 [D loss: 0.141775]\n",
      "1668 [D loss: 0.147390]\n",
      "1669 [D loss: 0.140370]\n",
      "1670 [D loss: 0.142239]\n",
      "1671 [D loss: 0.153156]\n",
      "1672 [D loss: 0.144748]\n",
      "1673 [D loss: 0.140415]\n",
      "1674 [D loss: 0.138757]\n",
      "1675 [D loss: 0.144943]\n",
      "1676 [D loss: 0.148894]\n",
      "1677 [D loss: 0.147459]\n",
      "1678 [D loss: 0.141286]\n",
      "1679 [D loss: 0.150865]\n",
      "1680 [D loss: 0.142698]\n",
      "1681 [D loss: 0.144262]\n",
      "1682 [D loss: 0.140527]\n",
      "1683 [D loss: 0.145704]\n",
      "1684 [D loss: 0.146775]\n",
      "1685 [D loss: 0.143460]\n",
      "1686 [D loss: 0.147482]\n",
      "1687 [D loss: 0.139747]\n",
      "1688 [D loss: 0.150422]\n",
      "1689 [D loss: 0.173070]\n",
      "1690 [D loss: 0.142024]\n",
      "1691 [D loss: 0.141060]\n",
      "1692 [D loss: 0.143875]\n",
      "1693 [D loss: 0.142740]\n",
      "1694 [D loss: 0.142108]\n",
      "1695 [D loss: 0.144664]\n",
      "1696 [D loss: 0.140955]\n",
      "1697 [D loss: 0.141147]\n",
      "1698 [D loss: 0.141634]\n",
      "1699 [D loss: 0.135473]\n",
      "1700 [D loss: 0.145777]\n",
      "1701 [D loss: 0.156191]\n",
      "1702 [D loss: 0.137796]\n",
      "1703 [D loss: 0.143411]\n",
      "1704 [D loss: 0.143712]\n",
      "1705 [D loss: 0.149044]\n",
      "1706 [D loss: 0.137608]\n",
      "1707 [D loss: 0.141618]\n",
      "1708 [D loss: 0.142222]\n",
      "1709 [D loss: 0.138072]\n",
      "1710 [D loss: 0.149212]\n",
      "1711 [D loss: 0.155179]\n",
      "1712 [D loss: 0.138379]\n",
      "1713 [D loss: 0.143657]\n",
      "1714 [D loss: 0.137650]\n",
      "1715 [D loss: 0.138166]\n",
      "1716 [D loss: 0.144886]\n",
      "1717 [D loss: 0.143358]\n",
      "1718 [D loss: 0.137997]\n",
      "1719 [D loss: 0.153924]\n",
      "1720 [D loss: 0.141954]\n",
      "1721 [D loss: 0.138666]\n",
      "1722 [D loss: 0.136620]\n",
      "1723 [D loss: 0.135679]\n",
      "1724 [D loss: 0.137479]\n",
      "1725 [D loss: 0.138819]\n",
      "1726 [D loss: 0.134969]\n",
      "1727 [D loss: 0.146840]\n",
      "1728 [D loss: 0.134773]\n",
      "1729 [D loss: 0.135087]\n",
      "1730 [D loss: 0.140613]\n",
      "1731 [D loss: 0.141979]\n",
      "1732 [D loss: 0.136617]\n",
      "1733 [D loss: 0.136476]\n",
      "1734 [D loss: 0.144050]\n",
      "1735 [D loss: 0.139515]\n",
      "1736 [D loss: 0.139543]\n",
      "1737 [D loss: 0.140828]\n",
      "1738 [D loss: 0.142928]\n",
      "1739 [D loss: 0.142951]\n",
      "1740 [D loss: 0.147527]\n",
      "1741 [D loss: 0.144422]\n",
      "1742 [D loss: 0.134204]\n",
      "1743 [D loss: 0.139086]\n",
      "1744 [D loss: 0.136782]\n",
      "1745 [D loss: 0.143793]\n",
      "1746 [D loss: 0.137359]\n",
      "1747 [D loss: 0.142359]\n",
      "1748 [D loss: 0.141233]\n",
      "1749 [D loss: 0.139486]\n",
      "1750 [D loss: 0.140463]\n",
      "1751 [D loss: 0.142111]\n",
      "1752 [D loss: 0.142733]\n",
      "1753 [D loss: 0.140884]\n",
      "1754 [D loss: 0.134157]\n",
      "1755 [D loss: 0.135710]\n",
      "1756 [D loss: 0.136057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1757 [D loss: 0.131459]\n",
      "1758 [D loss: 0.129239]\n",
      "1759 [D loss: 0.140539]\n",
      "1760 [D loss: 0.134377]\n",
      "1761 [D loss: 0.140450]\n",
      "1762 [D loss: 0.147052]\n",
      "1763 [D loss: 0.146238]\n",
      "1764 [D loss: 0.135368]\n",
      "1765 [D loss: 0.139108]\n",
      "1766 [D loss: 0.134915]\n",
      "1767 [D loss: 0.137447]\n",
      "1768 [D loss: 0.136226]\n",
      "1769 [D loss: 0.143368]\n",
      "1770 [D loss: 0.149929]\n",
      "1771 [D loss: 0.132010]\n",
      "1772 [D loss: 0.135586]\n",
      "1773 [D loss: 0.132458]\n",
      "1774 [D loss: 0.170353]\n",
      "1775 [D loss: 0.137787]\n",
      "1776 [D loss: 0.134929]\n",
      "1777 [D loss: 0.132476]\n",
      "1778 [D loss: 0.140131]\n",
      "1779 [D loss: 0.137273]\n",
      "1780 [D loss: 0.137344]\n",
      "1781 [D loss: 0.132264]\n",
      "1782 [D loss: 0.130964]\n",
      "1783 [D loss: 0.128787]\n",
      "1784 [D loss: 0.129708]\n",
      "1785 [D loss: 0.139605]\n",
      "1786 [D loss: 0.132390]\n",
      "1787 [D loss: 0.133864]\n",
      "1788 [D loss: 0.131308]\n",
      "1789 [D loss: 0.130535]\n",
      "1790 [D loss: 0.130730]\n",
      "1791 [D loss: 0.141824]\n",
      "1792 [D loss: 0.219446]\n",
      "1793 [D loss: 0.141172]\n",
      "1794 [D loss: 0.138501]\n",
      "1795 [D loss: 0.134218]\n",
      "1796 [D loss: 0.136000]\n",
      "1797 [D loss: 0.130675]\n",
      "1798 [D loss: 0.132550]\n",
      "1799 [D loss: 0.139083]\n",
      "1800 [D loss: 0.131892]\n",
      "1801 [D loss: 0.131447]\n",
      "1802 [D loss: 0.134859]\n",
      "1803 [D loss: 0.139663]\n",
      "1804 [D loss: 0.125957]\n",
      "1805 [D loss: 0.138615]\n",
      "1806 [D loss: 0.142443]\n",
      "1807 [D loss: 0.138059]\n",
      "1808 [D loss: 0.139094]\n",
      "1809 [D loss: 0.131129]\n",
      "1810 [D loss: 0.135748]\n",
      "1811 [D loss: 0.137012]\n",
      "1812 [D loss: 0.130189]\n",
      "1813 [D loss: 0.132264]\n",
      "1814 [D loss: 0.137600]\n",
      "1815 [D loss: 0.128879]\n",
      "1816 [D loss: 0.134352]\n",
      "1817 [D loss: 0.135777]\n",
      "1818 [D loss: 0.134937]\n",
      "1819 [D loss: 0.130779]\n",
      "1820 [D loss: 0.133728]\n",
      "1821 [D loss: 0.127243]\n",
      "1822 [D loss: 0.137444]\n",
      "1823 [D loss: 0.126850]\n",
      "1824 [D loss: 0.126296]\n",
      "1825 [D loss: 0.132561]\n",
      "1826 [D loss: 0.142036]\n",
      "1827 [D loss: 0.135863]\n",
      "1828 [D loss: 0.138486]\n",
      "1829 [D loss: 0.133855]\n",
      "1830 [D loss: 0.128638]\n",
      "1831 [D loss: 0.129721]\n",
      "1832 [D loss: 0.126375]\n",
      "1833 [D loss: 0.129784]\n",
      "1834 [D loss: 0.126847]\n",
      "1835 [D loss: 0.138431]\n",
      "1836 [D loss: 0.132390]\n",
      "1837 [D loss: 0.125116]\n",
      "1838 [D loss: 0.129763]\n",
      "1839 [D loss: 0.135401]\n",
      "1840 [D loss: 0.135395]\n",
      "1841 [D loss: 0.131204]\n",
      "1842 [D loss: 0.139337]\n",
      "1843 [D loss: 0.130549]\n",
      "1844 [D loss: 0.123970]\n",
      "1845 [D loss: 0.131097]\n",
      "1846 [D loss: 0.131565]\n",
      "1847 [D loss: 0.133890]\n",
      "1848 [D loss: 0.130371]\n",
      "1849 [D loss: 0.129959]\n",
      "1850 [D loss: 0.127879]\n",
      "1851 [D loss: 0.131080]\n",
      "1852 [D loss: 0.129449]\n",
      "1853 [D loss: 0.130293]\n",
      "1854 [D loss: 0.127912]\n",
      "1855 [D loss: 0.129918]\n",
      "1856 [D loss: 0.126435]\n",
      "1857 [D loss: 0.132139]\n",
      "1858 [D loss: 0.133931]\n",
      "1859 [D loss: 0.127525]\n",
      "1860 [D loss: 0.123763]\n",
      "1861 [D loss: 0.137636]\n",
      "1862 [D loss: 0.122099]\n",
      "1863 [D loss: 0.125629]\n",
      "1864 [D loss: 0.134922]\n",
      "1865 [D loss: 0.132775]\n",
      "1866 [D loss: 0.124138]\n",
      "1867 [D loss: 0.135479]\n",
      "1868 [D loss: 0.125234]\n",
      "1869 [D loss: 0.126568]\n",
      "1870 [D loss: 0.128883]\n",
      "1871 [D loss: 0.126254]\n",
      "1872 [D loss: 0.126132]\n",
      "1873 [D loss: 0.126555]\n",
      "1874 [D loss: 0.127883]\n",
      "1875 [D loss: 0.134138]\n",
      "1876 [D loss: 0.123776]\n",
      "1877 [D loss: 0.126094]\n",
      "1878 [D loss: 0.127790]\n",
      "1879 [D loss: 0.134843]\n",
      "1880 [D loss: 0.125852]\n",
      "1881 [D loss: 0.128613]\n",
      "1882 [D loss: 0.123570]\n",
      "1883 [D loss: 0.127259]\n",
      "1884 [D loss: 0.123887]\n",
      "1885 [D loss: 0.123405]\n",
      "1886 [D loss: 0.140387]\n",
      "1887 [D loss: 0.126097]\n",
      "1888 [D loss: 0.135038]\n",
      "1889 [D loss: 0.127155]\n",
      "1890 [D loss: 0.133733]\n",
      "1891 [D loss: 0.152838]\n",
      "1892 [D loss: 0.125391]\n",
      "1893 [D loss: 0.126455]\n",
      "1894 [D loss: 0.126066]\n",
      "1895 [D loss: 0.131421]\n",
      "1896 [D loss: 0.133710]\n",
      "1897 [D loss: 0.134877]\n",
      "1898 [D loss: 0.136015]\n",
      "1899 [D loss: 0.133280]\n",
      "1900 [D loss: 0.123159]\n",
      "1901 [D loss: 0.131859]\n",
      "1902 [D loss: 0.121211]\n",
      "1903 [D loss: 0.126758]\n",
      "1904 [D loss: 0.120050]\n",
      "1905 [D loss: 0.127285]\n",
      "1906 [D loss: 0.124150]\n",
      "1907 [D loss: 0.123778]\n",
      "1908 [D loss: 0.127384]\n",
      "1909 [D loss: 0.119120]\n",
      "1910 [D loss: 0.119347]\n",
      "1911 [D loss: 0.123115]\n",
      "1912 [D loss: 0.119033]\n",
      "1913 [D loss: 0.127606]\n",
      "1914 [D loss: 0.124507]\n",
      "1915 [D loss: 0.121432]\n",
      "1916 [D loss: 0.124424]\n",
      "1917 [D loss: 0.135280]\n",
      "1918 [D loss: 0.123910]\n",
      "1919 [D loss: 0.132845]\n",
      "1920 [D loss: 0.122219]\n",
      "1921 [D loss: 0.130500]\n",
      "1922 [D loss: 0.122998]\n",
      "1923 [D loss: 0.117793]\n",
      "1924 [D loss: 0.122486]\n",
      "1925 [D loss: 0.119364]\n",
      "1926 [D loss: 0.142076]\n",
      "1927 [D loss: 0.119710]\n",
      "1928 [D loss: 0.124651]\n",
      "1929 [D loss: 0.129565]\n",
      "1930 [D loss: 0.116419]\n",
      "1931 [D loss: 0.125848]\n",
      "1932 [D loss: 0.127249]\n",
      "1933 [D loss: 0.123666]\n",
      "1934 [D loss: 0.124710]\n",
      "1935 [D loss: 0.122444]\n",
      "1936 [D loss: 0.120146]\n",
      "1937 [D loss: 0.122906]\n",
      "1938 [D loss: 0.125257]\n",
      "1939 [D loss: 0.131142]\n",
      "1940 [D loss: 0.116680]\n",
      "1941 [D loss: 0.121526]\n",
      "1942 [D loss: 0.122181]\n",
      "1943 [D loss: 0.127624]\n",
      "1944 [D loss: 0.127702]\n",
      "1945 [D loss: 0.123379]\n",
      "1946 [D loss: 0.127042]\n",
      "1947 [D loss: 0.121029]\n",
      "1948 [D loss: 0.119157]\n",
      "1949 [D loss: 0.133306]\n",
      "1950 [D loss: 0.121392]\n",
      "1951 [D loss: 0.133616]\n",
      "1952 [D loss: 0.118207]\n",
      "1953 [D loss: 0.121258]\n",
      "1954 [D loss: 0.119280]\n",
      "1955 [D loss: 0.121110]\n",
      "1956 [D loss: 0.121206]\n",
      "1957 [D loss: 0.119879]\n",
      "1958 [D loss: 0.119777]\n",
      "1959 [D loss: 0.128343]\n",
      "1960 [D loss: 0.128509]\n",
      "1961 [D loss: 0.119507]\n",
      "1962 [D loss: 0.120027]\n",
      "1963 [D loss: 0.120363]\n",
      "1964 [D loss: 0.120458]\n",
      "1965 [D loss: 0.121797]\n",
      "1966 [D loss: 0.125358]\n",
      "1967 [D loss: 0.121432]\n",
      "1968 [D loss: 0.118941]\n",
      "1969 [D loss: 0.116217]\n",
      "1970 [D loss: 0.121983]\n",
      "1971 [D loss: 0.126956]\n",
      "1972 [D loss: 0.119935]\n",
      "1973 [D loss: 0.131369]\n",
      "1974 [D loss: 0.115956]\n",
      "1975 [D loss: 0.128393]\n",
      "1976 [D loss: 0.149790]\n",
      "1977 [D loss: 0.118863]\n",
      "1978 [D loss: 0.121093]\n",
      "1979 [D loss: 0.132486]\n",
      "1980 [D loss: 0.117330]\n",
      "1981 [D loss: 0.120058]\n",
      "1982 [D loss: 0.127721]\n",
      "1983 [D loss: 0.118022]\n",
      "1984 [D loss: 0.119367]\n",
      "1985 [D loss: 0.138766]\n",
      "1986 [D loss: 0.122404]\n",
      "1987 [D loss: 0.116130]\n",
      "1988 [D loss: 0.121481]\n",
      "1989 [D loss: 0.118912]\n",
      "1990 [D loss: 0.123827]\n",
      "1991 [D loss: 0.119512]\n",
      "1992 [D loss: 0.126562]\n",
      "1993 [D loss: 0.115080]\n",
      "1994 [D loss: 0.116912]\n",
      "1995 [D loss: 0.116522]\n",
      "1996 [D loss: 0.119736]\n",
      "1997 [D loss: 0.115215]\n",
      "1998 [D loss: 0.114514]\n",
      "1999 [D loss: 0.120263]\n"
     ]
    }
   ],
   "source": [
    "# Train it\n",
    "ae = AutoEncoder()\n",
    "ae.train(n_epochs=2000, batch_size=64, save_interval=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.AutoEncoder at 0x269e8389940>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae.img_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
