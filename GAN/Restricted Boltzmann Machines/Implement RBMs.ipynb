{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machines\n",
    "\n",
    "## Dataset \n",
    "\n",
    "Using ```MovieLens 1M Dataset```. During the training time the Restricted Boltzmann Machine learns on the first 5 movie ratings of each user, while during the inference time the model tries to predict the ratings for the last 5 movies. These predicted ratings are then compared with the actual ratings which were put into the test set.\n",
    "\n",
    "Both datasets are saved in a binary <i>```TFRecords```</i> format that enables a very efficient data input pipeline.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/669/1*7HtJjWXu8WJ4OApcC5ROIw.png\"/>\n",
    "\n",
    "## Model Architecture \n",
    "\n",
    "The model is implemented in an object oriented manner. The restricted RBMs is a class with all necessary operations like training, loss, accuracy, inference etc. Some helper functions are outsourced into a seperate script.\n",
    "\n",
    "The constructor sets the kernel initializers for the weights and biases. In the next step all weights and biases in the network get initialized. The weights are normal distributed with a ```mean of 0.0 and a variance of 0.02```, while the biases are all set to ```0.0``` in the beginning. It can be noticed that the network consists only out of one ```hidden layer```. As a result only ```one weight matrix``` is needed.\n",
    "\n",
    "## Sampling of the Hidden States\n",
    "<img src=\"https://miro.medium.com/max/791/1*Z8o2XfA7c6PSPrzShYaGZw.png\"/>\n",
    "\n",
    "Giving the binary input ```v``` the following function ```_sample_h(self)``` obtains the probabilities that a hidden neuron is activated (Eq.1). This is achieved by multiplying the input `v` by the `weight matrix`, adding a `bias` and applying a ```sigmoidal activation``` .\n",
    "\n",
    "The obtained probabilities are used to sample from ```Bernoulli distribution```. The sample values which are either ```1.0``` or `0.0` are the states of the `hidden` neurons.\n",
    "\n",
    "## Sampling the Visible States\n",
    "<img src=\"https://miro.medium.com/max/725/1*6BMmNqK8H3a_BFSq5K3j-A.png\"/>\n",
    "\n",
    "Given the hidden states `h` we can use these to obtain the probabilities that a visible neuron is `active` as well as the corresponding `state` values. This is implemented in `_sample_v(self)`\n",
    "\n",
    "## Gibbs Sampling \n",
    "<img src=\"https://miro.medium.com/max/875/1*UMbNSJVSmAgqkVnQKA62yg.png\"/>\n",
    "\n",
    "The first part of the training consists in an operation that is called <i><b>```Gibbs Sampling```</b></i>.Briefly speaking we take an <b>input vector</b> `v_0` and use it to predict the values of the <b>hidden state</b> `h_0`.\n",
    "\n",
    "The hidden state are used on the other hand to predict new input state `v`. The procedure is repeated <i>`k`</i> times.\n",
    "\n",
    "An important step while implementing `Gibbs Sampling` is  body is ```Vk=tf.where(tf.less(V,0),V,Vk)```. This operations makes sure that the ratings in `v` which are `-1` (meaning movies that have not been seen yet) remain `-1` for every ```v_k``` in every iteration. \n",
    "\n",
    "After `k` iteration we obtain ```v_k``` and corresponding probabilities ```p(h_k|v_k)```. Together with `v_0` and `h_0` these values can be used to compute the ```gradient matrix``` in the next training step.\n",
    "\n",
    "## Computing the Gradients\n",
    "\n",
    "The values obtained in the previous step can be used to compute the ```gradient matrix``` and the ```gradient vectors```.  The computation of gradients according to Eq. 3 are straight forward.\n",
    "\n",
    "To compute the gradients, reshaping and applying usual ```point wise``` multplication.\n",
    "<img src=\"https://miro.medium.com/max/814/1*lGBNVKoCsiWRmYfaVZGQTw.png\"/>\n",
    "\n",
    "Notice that the computation of the gradients is happening in while loop. This is only due to the fact that the training is happening in ```mini-batches```. Meaning the loop computes for each ```data sample in the mini-batch``` the gradients and adds them to the previously defined ```gradient placeholders```. In the end the sum of ```gradients``` is divided by the size of the ```mini-batch```.\n",
    "\n",
    "## Update Step\n",
    "\n",
    "After the gradients are computed all weights and biases can be updated through ```gradient ascent``` according to eq.4.\n",
    "For this procedure we must create an assign operation in ```_update_parameter(self)```.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/446/1*rCibXSBkhjpiX74GwFWzmQ.png\"/>\n",
    "\n",
    "The whole training operation is computed in ```optimize(self)``` method under the name scope ```“operation”```. Below that the more complicated accuracy operation of the training is implemented. Basically this operation subtracts the original input values `v_0` from `v_k` that are obtained during ```Gibbs Sampling```. The subtraction is only happening for `v_0 ≥ 0`. After that the summed subtractions are divided by the number of all ratings `≥` `0`. The accuracy gives the ```ratio of correctly``` predicted `binary` movie ratings during `training`.\n",
    "\n",
    "## Inference\n",
    "\n",
    "During inference time the method ```inference(self)``` receives the input `v`. That input is one training sample of a specific user that is used to activate the ```hidden neurons``` (the underlying features of users movie taste). The ```hidden neurons``` are used again to predict a new input `v`. In the best scenario this new input consists of the recreation of already present ratings as well as ratings of movies that were not rated yet.\n",
    "\n",
    "## The Network Graph\n",
    "\n",
    "To outline the previous steps here is the definition of the main ```network graph``` and the start of the ```session``` where the ```training``` and ```inference``` steps are executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Helper Functions\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "def _get_bias_init():\n",
    "    return tf.zeros_initializer()\n",
    "\n",
    "def _get_weight_init():\n",
    "    return tf.random_normal_initializer(mean=0.0, stddev=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBM Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM:\n",
    "    \"\"\"\n",
    "    Implementation of the Restricted Boltzmann Machine for collaborative filtering.\n",
    "    \"\"\"\n",
    "    def __init__(self, FLAGS):\n",
    "        ''' Initialization of the model '''\n",
    "        self.FLAGS = FLAGS\n",
    "        self.weight_init = _get_weight_init()\n",
    "        self.bias_init = _get_bias_init()\n",
    "        self.init_parameter()\n",
    "        \n",
    "    def init_parameter():\n",
    "        ''' Initializes the weights and the bias parameters of the neural network '''\n",
    "        with tf.variable_scope('Network_parameter'):\n",
    "            self.W=tf.get_variable('Weights', shape=(self.FLAGS.num_v, self.FLAGS.num_h),initializer=self.weight_initializer)\n",
    "            self.bh=tf.get_variable('hidden_bias', shape=(self.FLAGS.num_h), initializer=self.bias_initializer)\n",
    "            self.bv=tf.get_variable('visible_bias', shape=(self.FLAGS.num_v), initializer=self.bias_initializer)\n",
    "            \n",
    "    def _sample_h(self, v):\n",
    "        ''' Uses the visible nodes for calculation of  the probabilities that a hidden neuron is activated. \n",
    "        After that Bernouille distribution is used to sample the hidden nodes.\n",
    "        \n",
    "        @param v: visible nodes\n",
    "        @return probability that a hidden neuron is activated\n",
    "        @return sampled hidden neurons (value 1 or 0 accroding to Bernouille distribution)\n",
    "        '''  \n",
    "        with tf.name_scope('sampling_hidden_units'):\n",
    "            a=tf.nn.bias_add(tf.matmul(v,self.W), self.bh)\n",
    "            p_h_v=tf.nn.sigmoid(a)\n",
    "            h_=self._bernouille_sampling(p_h_v, shape=[self.FLAGS.batch_size, int(p_h_v.shape[-1])])\n",
    "            \n",
    "            return p_h_v, h_\n",
    "        \n",
    "    \n",
    "    def _sample_v(self, h):\n",
    "        ''' Uses the hidden nodes for calculation of  the probabilities that a visible neuron is activated. \n",
    "        After that Bernouille distribution is used to sample the visible nodes.\n",
    "        \n",
    "        @param h: hidden nodes\n",
    "        @return probability that a visible neuron is activated\n",
    "        @return sampled visible neurons (value 1 or 0 accroding to Bernouille distribution)\n",
    "        '''\n",
    "        \n",
    "        with tf.name_scope('sampling_visible_units'):\n",
    "            a=tf.nn.bias_add(tf.matmul(h,tf.transpose(self.W, [1,0])), self.bv)\n",
    "            p_v_h=tf.nn.sigmoid(a)\n",
    "            v_=self._bernouille_sampling(p_v_h, shape=[self.FLAGS.batch_size, int(p_v_h.shape[-1])])\n",
    "            \n",
    "            return p_v_h, v_\n",
    "        \n",
    "\n",
    "    def optimize(self, v):\n",
    "        ''' Optimization step. Gibbs sampling, calculating of gradients and doing an update operation.\n",
    "        \n",
    "        @param v: visible nodes\n",
    "        @return update operation\n",
    "        @return accuracy\n",
    "        '''\n",
    "\n",
    "        with tf.name_scope('optimization'):\n",
    "            v0, vk,ph0, phk, _=self._gibbs_sampling(v)\n",
    "            dW,db_h,db_v=self._compute_gradients(v0, vk, ph0, phk)\n",
    "            update_op =self._update_parameter(dW,db_h,db_v)\n",
    "        \n",
    "        with tf.name_scope('accuracy'):\n",
    "            mask=tf.where(tf.less(v0,0.0),x=tf.zeros_like(v0),y=tf.ones_like(v0))\n",
    "            bool_mask=tf.cast(tf.where(tf.less(v0,0.0),x=tf.zeros_like(v0),y=tf.ones_like(v0)), dtype=tf.bool)\n",
    "            acc=tf.where(bool_mask,x=tf.abs(tf.subtract(v0,vk)),y=tf.zeros_like(v0))\n",
    "            n_values=tf.reduce_sum(mask)\n",
    "            acc=tf.subtract(1.0,tf.div(tf.reduce_sum(acc), n_values))\n",
    "            \n",
    "            \n",
    "        return update_op, acc\n",
    "    \n",
    "    def inference(self, v):\n",
    "        '''Inference step. Training samples are used to activate the hidden neurons which are used for calculation of input neuron values.\n",
    "        This new input values are the prediction, for already rated movies as well as not yet rated movies\n",
    "        \n",
    "        @param v: visible nodes\n",
    "        @return sampled visible neurons (value 1 or 0 accroding to Bernouille distribution)\n",
    "        '''\n",
    "        p_h_v=tf.nn.sigmoid(tf.nn.bias_add(tf.matmul(v,self.W), self.bh))\n",
    "        h_=self._bernouille_sampling(p_h_v, shape=[1,int(p_h_v.shape[-1])])\n",
    "        \n",
    "        p_v_h=tf.nn.sigmoid(tf.nn.bias_add(tf.matmul(h_,tf.transpose(self.W, [1,0])), self.bv))\n",
    "        v_=self._bernouille_sampling(p_v_h, shape=[1,int(p_v_h.shape[-1])])\n",
    "        \n",
    "        return v_\n",
    "    \n",
    "    \n",
    "    def _update_parameter(self,dW,db_h,db_v):\n",
    "        ''' Creating TF assign operations. Updated weight and bias values are replacing old parameter values.\n",
    "        \n",
    "        @return assign operations\n",
    "        '''\n",
    "        \n",
    "        alpha=self.FLAGS.learning_rate\n",
    "        \n",
    "        update_op=[tf.assign(self.W, alpha*tf.add(self.W,dW)),\n",
    "                   tf.assign(self.bh, alpha*tf.add(self.bh,db_h)),\n",
    "                   tf.assign(self.bv, alpha*tf.add(self.bv,db_v))]\n",
    "\n",
    "        return update_op\n",
    "    \n",
    "    \n",
    "    def _compute_gradients(self,v0, vk, ph0, phk):\n",
    "        ''' Computing the gradients of the weights and bias terms with Contrastive Divergence.\n",
    "        \n",
    "        @param v0: visible neurons before gibbs sampling\n",
    "        @param vk: visible neurons after gibbs sampling\n",
    "        @param ph0: probability that hidden neurons are activated before gibbs sampling.\n",
    "        @param phk: probability that hidden neurons are activated after gibbs sampling.\n",
    "        \n",
    "        @return gradients of the network parameters\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #end condition for the while loop\n",
    "        def condition(i, v0, vk, ph0, phk, dW,db_h,db_v):\n",
    "            r=tf.less(i,k)\n",
    "            return r[0]\n",
    "        \n",
    "        #loop body\n",
    "        def body(i, v0, vk, ph0, phk, dW,dbh,dbv):\n",
    "            \n",
    "            v0_=v0[i]\n",
    "            ph0_=ph0[i]\n",
    "            \n",
    "            vk_=vk[i]\n",
    "            phk_=phk[i]       \n",
    "            \n",
    "            #reshaping for making the outer product possible\n",
    "            ph0_=tf.reshape(ph0_, [1,self.FLAGS.num_h])\n",
    "            v0_=tf.reshape(v0_, [self.FLAGS.num_v,1])\n",
    "            phk_=tf.reshape(phk_, [1,self.FLAGS.num_h])\n",
    "            vk_=tf.reshape(vk_, [self.FLAGS.num_v,1])\n",
    "            \n",
    "            #calculating the gradiends for weights and biases\n",
    "            dw_=tf.subtract(tf.multiply(ph0_, v0_),tf.multiply(phk_, vk_))\n",
    "            dbh_=tf.subtract(ph0_,phk_)\n",
    "            dbv_=tf.subtract(v0_,vk_)\n",
    "            \n",
    "            dbh_=tf.reshape(dbh_,[self.FLAGS.num_h])\n",
    "            dbv_=tf.reshape(dbv_,[self.FLAGS.num_v])\n",
    "            \n",
    "            return [i+1, v0, vk, ph0, phk,tf.add(dW,dw_),tf.add(dbh,dbh_),tf.add(dbv,dbv_)]\n",
    "        \n",
    "        i = 0 # start counter for the while loop\n",
    "        k=tf.constant([self.FLAGS.batch_size]) # number for the end condition of the while loop\n",
    "        \n",
    "        #init empty placeholders wherer the gradients will be stored              \n",
    "        dW=tf.zeros((self.FLAGS.num_v, self.FLAGS.num_h))\n",
    "        dbh=tf.zeros((self.FLAGS.num_h))\n",
    "        dbv=tf.zeros((self.FLAGS.num_v))\n",
    "        \n",
    "        #iterate over the batch and compute for each sample a gradient\n",
    "        [i, v0, vk, ph0, phk, dW,db_h,db_v]=tf.while_loop(condition, body,[i, v0, vk, ph0, phk, dW,dbh,dbv])\n",
    "          \n",
    "        #devide the summed gradiends by the batch size\n",
    "        dW=tf.div(dW, self.FLAGS.batch_size)\n",
    "        dbh=tf.div(dbh, self.FLAGS.batch_size)\n",
    "        dbv=tf.div(dbv, self.FLAGS.batch_size)\n",
    "        \n",
    "        return dW,dbh,dbv\n",
    "        \n",
    "    \n",
    "    def _gibbs_sampling(self, v):\n",
    "        ''' Perfroming the gibbs sampling.\n",
    "        \n",
    "        @param v: visible neurons\n",
    "        @return visible neurons before gibbs sampling\n",
    "        @return visible neurons before gibbs sampling\n",
    "        @return probability that hidden neurons are activated before gibbs sampling.\n",
    "        @return probability that hidden neurons are activated after gibbs sampling.\n",
    "        '''\n",
    " \n",
    "        #end condition for the while loop\n",
    "        def condition(i, vk, hk,v):\n",
    "            r= tf.less(i,k)\n",
    "            return r[0]\n",
    "        \n",
    "        #loop body\n",
    "        def body(i, vk, hk,v):\n",
    "            \n",
    "            _,hk=self._sample_h(vk)\n",
    "            _,vk=self._sample_v(hk)\n",
    "\n",
    "            vk=tf.where(tf.less(v,0),v,vk)\n",
    "            \n",
    "            return [i+1, vk, hk,v]\n",
    "            \n",
    "        ph0,_=self._sample_h(v)\n",
    "        \n",
    "        vk = v\n",
    "        hk = tf.zeros_like(ph0)\n",
    "            \n",
    "        i = 0 # start counter for the while loop\n",
    "        k = tf.constant([self.FLAGS.k]) # number for the end condition of the while loop\n",
    "        \n",
    "        [i, vk,hk,v] = tf.while_loop(condition, body,[i, vk,hk,v])\n",
    "        \n",
    "        phk, _ = self._sample_h(vk)\n",
    "        \n",
    "        return v, vk,ph0, phk, i\n",
    "        \n",
    "    \n",
    "    def _bernouille_sampling(self,p, shape):\n",
    "        '''Samples from the Bernoulli distribution\n",
    "        \n",
    "        @param p: probability \n",
    "        @return samples from Bernoulli distribution\n",
    "        \n",
    "        '''\n",
    "        return tf.where(tf.less(p, tf.random_uniform(shape,minval=0.0,maxval=1.0)),\n",
    "                        x=tf.zeros_like(p),\n",
    "                        y=tf.ones_like(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
